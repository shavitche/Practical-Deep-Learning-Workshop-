{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"part2-Building models.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"tppMGDpTcxjV","colab_type":"text"},"cell_type":"markdown","source":["we use this snippet to display dataframes side by side:"]},{"metadata":{"id":"aATpdZDKcxjX","colab_type":"code","outputId":"09140787-ba74-4dad-fb10-aa22189340e6","colab":{}},"cell_type":"code","source":["from IPython.display import display, HTML\n","\n","CSS = \"\"\"\n",".output {\n","    flex-direction: row;\n","}\n","\"\"\"\n","\n","HTML('<style>{}</style>'.format(CSS))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<style>\n",".output {\n","    flex-direction: row;\n","}\n","</style>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":459}]},{"metadata":{"id":"moWsD-gWcxjd","colab_type":"text"},"cell_type":"markdown","source":["# Part 2  \n","### Reminder- our goal is to predict the next hour total consumption ('Global_active_power') in assumption we have the last X history hours features.  \n","let's load the train data and prepare it to use for our goal:  \n","(we read the date as datetime format, in order to use resampling later.)\n"]},{"metadata":{"id":"mK_WX5APcxje","colab_type":"code","outputId":"41719b5c-7272-4960-e530-5080ab1cc009","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","%pylab inline \n","\n","train_data = pd.read_csv('train_data.csv',sep=',', parse_dates=['Date'], infer_datetime_format=True, index_col = ['Date'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Populating the interactive namespace from numpy and matplotlib\n"],"name":"stdout"}]},{"metadata":{"scrolled":true,"id":"EKJ7Cy9Acxji","colab_type":"code","outputId":"76808b2b-82c6-4189-8406-f869a4f024f2","colab":{}},"cell_type":"code","source":["train_data.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Global_active_power</th>\n","      <th>Global_reactive_power</th>\n","      <th>Voltage</th>\n","      <th>Global_intensity</th>\n","      <th>Sub_metering_1</th>\n","      <th>Sub_metering_2</th>\n","      <th>Sub_metering_3</th>\n","    </tr>\n","    <tr>\n","      <th>Date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2006-12-16 17:24:00</th>\n","      <td>4.216</td>\n","      <td>0.418</td>\n","      <td>234.84</td>\n","      <td>18.4</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>17.0</td>\n","    </tr>\n","    <tr>\n","      <th>2006-12-16 17:25:00</th>\n","      <td>5.360</td>\n","      <td>0.436</td>\n","      <td>233.63</td>\n","      <td>23.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>16.0</td>\n","    </tr>\n","    <tr>\n","      <th>2006-12-16 17:26:00</th>\n","      <td>5.374</td>\n","      <td>0.498</td>\n","      <td>233.29</td>\n","      <td>23.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>17.0</td>\n","    </tr>\n","    <tr>\n","      <th>2006-12-16 17:27:00</th>\n","      <td>5.388</td>\n","      <td>0.502</td>\n","      <td>233.74</td>\n","      <td>23.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>17.0</td>\n","    </tr>\n","    <tr>\n","      <th>2006-12-16 17:28:00</th>\n","      <td>3.666</td>\n","      <td>0.528</td>\n","      <td>235.68</td>\n","      <td>15.8</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>17.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     Global_active_power  Global_reactive_power  Voltage  \\\n","Date                                                                       \n","2006-12-16 17:24:00                4.216                  0.418   234.84   \n","2006-12-16 17:25:00                5.360                  0.436   233.63   \n","2006-12-16 17:26:00                5.374                  0.498   233.29   \n","2006-12-16 17:27:00                5.388                  0.502   233.74   \n","2006-12-16 17:28:00                3.666                  0.528   235.68   \n","\n","                     Global_intensity  Sub_metering_1  Sub_metering_2  \\\n","Date                                                                    \n","2006-12-16 17:24:00              18.4             0.0             1.0   \n","2006-12-16 17:25:00              23.0             0.0             1.0   \n","2006-12-16 17:26:00              23.0             0.0             2.0   \n","2006-12-16 17:27:00              23.0             0.0             1.0   \n","2006-12-16 17:28:00              15.8             0.0             1.0   \n","\n","                     Sub_metering_3  \n","Date                                 \n","2006-12-16 17:24:00            17.0  \n","2006-12-16 17:25:00            16.0  \n","2006-12-16 17:26:00            17.0  \n","2006-12-16 17:27:00            17.0  \n","2006-12-16 17:28:00            17.0  "]},"metadata":{"tags":[]},"execution_count":265}]},{"metadata":{"id":"yklIHwiFcxjn","colab_type":"text"},"cell_type":"markdown","source":["### fill missing values  \n","we will fill missing data by the mean of the same row (same time different days) of all the days before and after (that are not Nan). "]},{"metadata":{"id":"2URu3WfPcxjo","colab_type":"code","colab":{}},"cell_type":"code","source":["def calculate_mean(data,r,c):\n","    acc = 0\n","    z=r\n","    count=0\n","    r -= 24*60\n","    while(r>0):\n","        if not np.isnan(data[r,c]):\n","            acc += data[r,c]\n","            count += 1\n","        r -= 24*60\n","    z += 24*60\n","    while(z<data.shape[0]):\n","        if not np.isnan(data[z,c]):\n","            acc += data[z,c]\n","            count += 1\n","        z += 24*60\n","    return acc/count\n","        \n","    \n","def fill_missing_values(data):\n","    data = data.values\n","    for r in range(data.shape[0]):\n","        for c in range(data.shape[1]):\n","            if np.isnan(data[r,c]):\n","                data[r,c] = calculate_mean1(data,r,c)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mLWPrkUAcxjq","colab_type":"code","colab":{}},"cell_type":"code","source":["fill_missing_values(train_data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rGvEyIJ9cxju","colab_type":"text"},"cell_type":"markdown","source":["check that there is no more missing values:"]},{"metadata":{"id":"wXwcC1Vhcxjv","colab_type":"code","outputId":"7bf3d19b-b433-4428-cdfd-c71f247fe860","colab":{}},"cell_type":"code","source":["train_data.isnull().any()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Global_active_power      False\n","Global_reactive_power    False\n","Voltage                  False\n","Global_intensity         False\n","Sub_metering_1           False\n","Sub_metering_2           False\n","Sub_metering_3           False\n","dtype: bool"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"id":"0r8pDt6Pcxjz","colab_type":"text"},"cell_type":"markdown","source":["the distribution has to be almost the same as in part 1:  \n","(this chart is helping has to ensure that 'fill_missing_values' worked correctly)"]},{"metadata":{"scrolled":true,"id":"0NM7s7qmcxj0","colab_type":"code","outputId":"5774a99c-23ee-429d-daa5-c1d9247d0996","colab":{}},"cell_type":"code","source":["train_data.describe().transpose()[['count','min','max','mean','std']]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","      <th>min</th>\n","      <th>max</th>\n","      <th>mean</th>\n","      <th>std</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Global_active_power</th>\n","      <td>1624716.0</td>\n","      <td>0.076</td>\n","      <td>11.122</td>\n","      <td>1.103249</td>\n","      <td>1.087804</td>\n","    </tr>\n","    <tr>\n","      <th>Global_reactive_power</th>\n","      <td>1624716.0</td>\n","      <td>0.000</td>\n","      <td>1.390</td>\n","      <td>0.122139</td>\n","      <td>0.111783</td>\n","    </tr>\n","    <tr>\n","      <th>Voltage</th>\n","      <td>1624716.0</td>\n","      <td>223.200</td>\n","      <td>254.150</td>\n","      <td>240.692599</td>\n","      <td>3.330322</td>\n","    </tr>\n","    <tr>\n","      <th>Global_intensity</th>\n","      <td>1624716.0</td>\n","      <td>0.200</td>\n","      <td>48.400</td>\n","      <td>4.681047</td>\n","      <td>4.573523</td>\n","    </tr>\n","    <tr>\n","      <th>Sub_metering_1</th>\n","      <td>1624716.0</td>\n","      <td>0.000</td>\n","      <td>82.000</td>\n","      <td>1.162230</td>\n","      <td>6.275138</td>\n","    </tr>\n","    <tr>\n","      <th>Sub_metering_2</th>\n","      <td>1624716.0</td>\n","      <td>0.000</td>\n","      <td>78.000</td>\n","      <td>1.353865</td>\n","      <td>5.994816</td>\n","    </tr>\n","    <tr>\n","      <th>Sub_metering_3</th>\n","      <td>1624716.0</td>\n","      <td>0.000</td>\n","      <td>31.000</td>\n","      <td>6.260199</td>\n","      <td>8.334372</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                           count      min      max        mean       std\n","Global_active_power    1624716.0    0.076   11.122    1.103249  1.087804\n","Global_reactive_power  1624716.0    0.000    1.390    0.122139  0.111783\n","Voltage                1624716.0  223.200  254.150  240.692599  3.330322\n","Global_intensity       1624716.0    0.200   48.400    4.681047  4.573523\n","Sub_metering_1         1624716.0    0.000   82.000    1.162230  6.275138\n","Sub_metering_2         1624716.0    0.000   78.000    1.353865  5.994816\n","Sub_metering_3         1624716.0    0.000   31.000    6.260199  8.334372"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"h1LFdwricxj4","colab_type":"text"},"cell_type":"markdown","source":["let's save the data:"]},{"metadata":{"id":"g2b_7Z-dcxj6","colab_type":"code","colab":{}},"cell_type":"code","source":["train_data.to_csv('data_without_nan.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cZTOxt2zcxj9","colab_type":"text"},"cell_type":"markdown","source":["### rearrange the data by hours  \n","first we will use resample to group all the rows of the same hour and sum each feature consumption to one row that represent the one hour consumption."]},{"metadata":{"id":"QNt0N2cucxj-","colab_type":"code","colab":{}},"cell_type":"code","source":["train_data_by_hours = train_data.resample('H').sum()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9UzhaKpecxkB","colab_type":"code","outputId":"a0d76532-08c4-4763-d1ac-5e5f5b687115","colab":{}},"cell_type":"code","source":["train_data_by_hours.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Global_active_power</th>\n","      <th>Global_reactive_power</th>\n","      <th>Voltage</th>\n","      <th>Global_intensity</th>\n","      <th>Sub_metering_1</th>\n","      <th>Sub_metering_2</th>\n","      <th>Sub_metering_3</th>\n","    </tr>\n","    <tr>\n","      <th>Date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2006-12-16 17:00:00</th>\n","      <td>152.024</td>\n","      <td>8.244</td>\n","      <td>8447.18</td>\n","      <td>651.6</td>\n","      <td>0.0</td>\n","      <td>19.0</td>\n","      <td>607.0</td>\n","    </tr>\n","    <tr>\n","      <th>2006-12-16 18:00:00</th>\n","      <td>217.932</td>\n","      <td>4.802</td>\n","      <td>14074.81</td>\n","      <td>936.0</td>\n","      <td>0.0</td>\n","      <td>403.0</td>\n","      <td>1012.0</td>\n","    </tr>\n","    <tr>\n","      <th>2006-12-16 19:00:00</th>\n","      <td>204.014</td>\n","      <td>5.114</td>\n","      <td>13993.95</td>\n","      <td>870.2</td>\n","      <td>0.0</td>\n","      <td>86.0</td>\n","      <td>1001.0</td>\n","    </tr>\n","    <tr>\n","      <th>2006-12-16 20:00:00</th>\n","      <td>196.114</td>\n","      <td>4.506</td>\n","      <td>14044.29</td>\n","      <td>835.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1007.0</td>\n","    </tr>\n","    <tr>\n","      <th>2006-12-16 21:00:00</th>\n","      <td>183.388</td>\n","      <td>4.600</td>\n","      <td>14229.52</td>\n","      <td>782.8</td>\n","      <td>0.0</td>\n","      <td>25.0</td>\n","      <td>1033.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     Global_active_power  Global_reactive_power   Voltage  \\\n","Date                                                                        \n","2006-12-16 17:00:00              152.024                  8.244   8447.18   \n","2006-12-16 18:00:00              217.932                  4.802  14074.81   \n","2006-12-16 19:00:00              204.014                  5.114  13993.95   \n","2006-12-16 20:00:00              196.114                  4.506  14044.29   \n","2006-12-16 21:00:00              183.388                  4.600  14229.52   \n","\n","                     Global_intensity  Sub_metering_1  Sub_metering_2  \\\n","Date                                                                    \n","2006-12-16 17:00:00             651.6             0.0            19.0   \n","2006-12-16 18:00:00             936.0             0.0           403.0   \n","2006-12-16 19:00:00             870.2             0.0            86.0   \n","2006-12-16 20:00:00             835.0             0.0             0.0   \n","2006-12-16 21:00:00             782.8             0.0            25.0   \n","\n","                     Sub_metering_3  \n","Date                                 \n","2006-12-16 17:00:00           607.0  \n","2006-12-16 18:00:00          1012.0  \n","2006-12-16 19:00:00          1001.0  \n","2006-12-16 20:00:00          1007.0  \n","2006-12-16 21:00:00          1033.0  "]},"metadata":{"tags":[]},"execution_count":113}]},{"metadata":{"id":"gJTLuYp4cxkI","colab_type":"text"},"cell_type":"markdown","source":["as we can see, each row in the dataframe represent an hour."]},{"metadata":{"id":"Icu7J5ZccxkJ","colab_type":"code","outputId":"bcd71829-d1f4-4cfe-8a5a-6e3d03bb1242","colab":{}},"cell_type":"code","source":["train_data_by_hours.describe().transpose()[['count','min','max','mean','std']]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","      <th>min</th>\n","      <th>max</th>\n","      <th>mean</th>\n","      <th>std</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Global_active_power</th>\n","      <td>27079.0</td>\n","      <td>7.440</td>\n","      <td>393.632</td>\n","      <td>66.193986</td>\n","      <td>55.566029</td>\n","    </tr>\n","    <tr>\n","      <th>Global_reactive_power</th>\n","      <td>27079.0</td>\n","      <td>1.262</td>\n","      <td>46.460</td>\n","      <td>7.328227</td>\n","      <td>4.020609</td>\n","    </tr>\n","    <tr>\n","      <th>Voltage</th>\n","      <td>27079.0</td>\n","      <td>8447.180</td>\n","      <td>15114.120</td>\n","      <td>14441.342638</td>\n","      <td>189.326071</td>\n","    </tr>\n","    <tr>\n","      <th>Global_intensity</th>\n","      <td>27079.0</td>\n","      <td>30.200</td>\n","      <td>1703.000</td>\n","      <td>280.858643</td>\n","      <td>232.739047</td>\n","    </tr>\n","    <tr>\n","      <th>Sub_metering_1</th>\n","      <td>27079.0</td>\n","      <td>0.000</td>\n","      <td>2902.000</td>\n","      <td>69.732769</td>\n","      <td>216.992509</td>\n","    </tr>\n","    <tr>\n","      <th>Sub_metering_2</th>\n","      <td>27079.0</td>\n","      <td>0.000</td>\n","      <td>2786.000</td>\n","      <td>81.230705</td>\n","      <td>260.229121</td>\n","    </tr>\n","    <tr>\n","      <th>Sub_metering_3</th>\n","      <td>27079.0</td>\n","      <td>0.000</td>\n","      <td>1227.000</td>\n","      <td>375.606410</td>\n","      <td>436.944765</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         count       min        max          mean         std\n","Global_active_power    27079.0     7.440    393.632     66.193986   55.566029\n","Global_reactive_power  27079.0     1.262     46.460      7.328227    4.020609\n","Voltage                27079.0  8447.180  15114.120  14441.342638  189.326071\n","Global_intensity       27079.0    30.200   1703.000    280.858643  232.739047\n","Sub_metering_1         27079.0     0.000   2902.000     69.732769  216.992509\n","Sub_metering_2         27079.0     0.000   2786.000     81.230705  260.229121\n","Sub_metering_3         27079.0     0.000   1227.000    375.606410  436.944765"]},"metadata":{"tags":[]},"execution_count":114}]},{"metadata":{"id":"6iFyLwJ1cxkO","colab_type":"text"},"cell_type":"markdown","source":["### split the train data to train and validation sets.  "]},{"metadata":{"id":"6fKLyYXDcxkP","colab_type":"text"},"cell_type":"markdown","source":["by this function we will save and load the data:"]},{"metadata":{"id":"_MBuCj63cxkQ","colab_type":"code","colab":{}},"cell_type":"code","source":["import pickle\n","def save_obj(obj,path):\n","    with open(path + '.pkl', \"wb\") as output:\n","        pickle.dump(obj, output)\n","def load_obj(path):\n","    with open(path + '.pkl', \"rb\") as output:\n","        return pickle.load(output)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BKXYlSyEcxkU","colab_type":"text"},"cell_type":"markdown","source":["we will use 80% from the data for training, and 20% for validation. because it is time series data we will not use any shuffle methods."]},{"metadata":{"id":"lLe25hy9cxkV","colab_type":"code","colab":{}},"cell_type":"code","source":["split = 0.8\n","q = round(split*len(train_data_by_hours))\n","train = train_data_by_hours[:q]\n","save_obj(train,'splited_data/train')\n","validation = train_data_by_hours[q:]\n","save_obj(validation,'splited_data/validation')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"w0ml1EB5cxka","colab_type":"code","outputId":"40a62ed1-3051-434e-9d53-c2dcd6d8837a","colab":{}},"cell_type":"code","source":["train.shape,validation.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((21663, 7), (5416, 7))"]},"metadata":{"tags":[]},"execution_count":117}]},{"metadata":{"id":"uhBAs5GLcxkf","colab_type":"text"},"cell_type":"markdown","source":["notice that there are 21663 hours in the training set and 5416 in the validation set."]},{"metadata":{"id":"yOia3XwRcxkf","colab_type":"text"},"cell_type":"markdown","source":["### how we evaluate our model?  \n","in order to evaluate our model and compare between different models we have to decide about 1 numeric value that represent our model quality. we will do it by rmse."]},{"metadata":{"id":"oiIvem4dcxkg","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.metrics import mean_squared_error as mse\n","from math import sqrt\n","\n","def rmse(actual,predicted):\n","    return sqrt(mse(actual,predicted))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IP6NPiBIcxki","colab_type":"text"},"cell_type":"markdown","source":["### series to supervised"]},{"metadata":{"id":"Vcz_7Ax1cxkk","colab_type":"text"},"cell_type":"markdown","source":["as we studied at lecture we have to convert the data from series data (sequences) , to supervised data in consider to our research question. in our case we have to take X sequences, and label them with the X+1 sequence column 'Global_active_power'. in this case, we decided not to use horizon. "]},{"metadata":{"id":"djkhFxbNcxkk","colab_type":"code","colab":{}},"cell_type":"code","source":["def to_supervised(data, n_input, n_out=1):\n","    X,Y = list(),list()\n","    in_start = 0\n","    # step over the entire history one time step at a time\n","    for _ in range(len(data)):\n","        # define the end of the input sequence\n","        in_end = in_start + n_input\n","        out_end = in_end + n_out\n","        # ensure we have enough data for this instance\n","        if out_end < len(data):\n","            X.append(data.iloc[in_start:in_end,:].values)\n","            Y.append(data.iloc[in_end:out_end,0].values[0])\n","        # move along one time step\n","        in_start += 1\n","    return X,Y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tLR0iAQWcxkn","colab_type":"text"},"cell_type":"markdown","source":["### normalize  \n","we have to normalize the validation and test data with the same parameters as the train data, we will do that by min max normalization with the following method:"]},{"metadata":{"id":"riPGLHRBcxko","colab_type":"code","colab":{}},"cell_type":"code","source":["def normalize(train,test):\n","    normalized_data = pd.DataFrame.copy(test)\n","    for i in normalized_data.columns[:]:\n","        normalized_data[i] = (normalized_data[i] - train[i].min()) / (train[i].max() - train[i].min())\n","    return normalized_data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dCMNivZocxkq","colab_type":"code","colab":{}},"cell_type":"code","source":["train_normalized = normalize(train, train)\n","validation_normalized = normalize(train, validation)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jf_JUv_ycxkt","colab_type":"code","outputId":"e5caef49-c424-4cfa-b280-d4d4ee10c1e8","colab":{}},"cell_type":"code","source":["train_normalized.describe().transpose()[['count','min','max','mean','std']]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","      <th>min</th>\n","      <th>max</th>\n","      <th>mean</th>\n","      <th>std</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Global_active_power</th>\n","      <td>21663.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.155583</td>\n","      <td>0.147516</td>\n","    </tr>\n","    <tr>\n","      <th>Global_reactive_power</th>\n","      <td>21663.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.150192</td>\n","      <td>0.101190</td>\n","    </tr>\n","    <tr>\n","      <th>Voltage</th>\n","      <td>21663.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.896260</td>\n","      <td>0.029005</td>\n","    </tr>\n","    <tr>\n","      <th>Global_intensity</th>\n","      <td>21663.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.153277</td>\n","      <td>0.142887</td>\n","    </tr>\n","    <tr>\n","      <th>Sub_metering_1</th>\n","      <td>21663.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.024646</td>\n","      <td>0.076123</td>\n","    </tr>\n","    <tr>\n","      <th>Sub_metering_2</th>\n","      <td>21663.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.030631</td>\n","      <td>0.098159</td>\n","    </tr>\n","    <tr>\n","      <th>Sub_metering_3</th>\n","      <td>21663.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.306640</td>\n","      <td>0.364152</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         count  min  max      mean       std\n","Global_active_power    21663.0  0.0  1.0  0.155583  0.147516\n","Global_reactive_power  21663.0  0.0  1.0  0.150192  0.101190\n","Voltage                21663.0  0.0  1.0  0.896260  0.029005\n","Global_intensity       21663.0  0.0  1.0  0.153277  0.142887\n","Sub_metering_1         21663.0  0.0  1.0  0.024646  0.076123\n","Sub_metering_2         21663.0  0.0  1.0  0.030631  0.098159\n","Sub_metering_3         21663.0  0.0  1.0  0.306640  0.364152"]},"metadata":{"tags":[]},"execution_count":129}]},{"metadata":{"id":"hW9RMds9cxkw","colab_type":"text"},"cell_type":"markdown","source":["we can notice that 'Global_active_power' values range are between [0,1] , the reason we pay attention to it , is in order to understand which values are reasonable for RMSE."]},{"metadata":{"id":"MJI_yt9-cxkx","colab_type":"text"},"cell_type":"markdown","source":["## Building machine learning for our goal"]},{"metadata":{"id":"GSBaQ4Tkcxky","colab_type":"text"},"cell_type":"markdown","source":["we will make the following steps in order to build and evaluate different machine learning and understand which one is the best:  \n","* each ml train on the train set.\n","* for each ml there are different parameters we can tune, in order to evaluate the best model among them, we will use the validation set. (so there is dependency between the model and the parameter we try to tune)\n","* after we choose the best tuning parameter, we want to evaluate the model with data that there is not any dependency between the model and the data, for this purpose we will use the test set.  \n","  \n","  \n","the train and validation set are ready to use let's do the same process with the test set:"]},{"metadata":{"id":"zKxWAYgPcxky","colab_type":"code","colab":{}},"cell_type":"code","source":["test_data = pd.read_csv('test_data.csv',sep=',', parse_dates=['Date'], infer_datetime_format=True, index_col = ['Date'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i2paqiavcxk0","colab_type":"code","outputId":"897422c6-58be-42ec-d022-c9edbb66f0f9","colab":{}},"cell_type":"code","source":["test_data.isnull().any()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Global_active_power      True\n","Global_reactive_power    True\n","Voltage                  True\n","Global_intensity         True\n","Sub_metering_1           True\n","Sub_metering_2           True\n","Sub_metering_3           True\n","dtype: bool"]},"metadata":{"tags":[]},"execution_count":59}]},{"metadata":{"id":"1mJw28i2cxk3","colab_type":"code","colab":{}},"cell_type":"code","source":["fill_missing_values(test_data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hc-Xi5Xycxk7","colab_type":"code","outputId":"e33965ce-068b-4905-bb74-4fdffcc2fdf4","colab":{}},"cell_type":"code","source":["test_data.isnull().any()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Global_active_power      False\n","Global_reactive_power    False\n","Voltage                  False\n","Global_intensity         False\n","Sub_metering_1           False\n","Sub_metering_2           False\n","Sub_metering_3           False\n","dtype: bool"]},"metadata":{"tags":[]},"execution_count":24}]},{"metadata":{"id":"YK9m_h-vcxk_","colab_type":"code","colab":{}},"cell_type":"code","source":["test_data_by_hours = test_data.resample('H').sum()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m94tLkFVcxlB","colab_type":"code","colab":{}},"cell_type":"code","source":["test_data_normalized = normalize(train,test_data_by_hours)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cwzfXiLGcxlE","colab_type":"text"},"cell_type":"markdown","source":["## Naive baseline"]},{"metadata":{"id":"LARvtj5AcxlE","colab_type":"text"},"cell_type":"markdown","source":["in order to create naive baseline model we have to choose a rule, we chose the following rule: 'our next hour prediction ('Global active power') will be the mean of X last hour global active power', the following baseline rule will do that:"]},{"metadata":{"id":"ZEvJdUORcxlF","colab_type":"code","colab":{}},"cell_type":"code","source":["def baseline_rule(data):\n","    sum = 0\n","    for el in data:\n","        sum += el[0]\n","    return sum/len(data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rHR081mQcxlI","colab_type":"text"},"cell_type":"markdown","source":["let's try to predict by using the last 9 hours for prediction:"]},{"metadata":{"id":"tAnEgouucxlJ","colab_type":"code","outputId":"716c96bc-f1e5-4190-cc5b-870669f2e278","colab":{}},"cell_type":"code","source":["n_in = 8\n","X,Y = to_supervised(validation_normalized,n_in)\n","prediction=[]\n","for x in X:\n","    prediction.append(baseline_rule(x))\n","rmse(Y,prediction)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.13098081870169129"]},"metadata":{"tags":[]},"execution_count":558}]},{"metadata":{"id":"EZ_-IAVlcxlL","colab_type":"text"},"cell_type":"markdown","source":["as we can see the result is not bad for baseline, the rmse is ~0.131 and the values range is between [0,1].  \n","because for baseline model we are satisfied with this result we will not try to check if with different n_in parameter (number of history hours). but we have to check the result on our test set, because on this set we will examine our benchmark:"]},{"metadata":{"id":"xxXgSHaycxlM","colab_type":"code","outputId":"c8284cc2-c7b7-4e00-a725-b5ea692954f6","colab":{}},"cell_type":"code","source":["X,Y = to_supervised(test_data_normalized,n_in)\n","prediction=[]\n","for x in X:\n","    prediction.append(baseline_rule(x))\n","rmse(Y,prediction)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.12071760352169737"]},"metadata":{"tags":[]},"execution_count":559}]},{"metadata":{"id":"HPBLSWjMcxlT","colab_type":"text"},"cell_type":"markdown","source":["as we can see that there is a little gap between the results , but because it's naive baseline this will not bother us for now. if in our next ml model we will get results with bigger gap we will have to check our split to validation set cause probably it is not depict the test set well.  \n","*** our baseline benchmark is 0.1200 ***"]},{"metadata":{"id":"H2U8l6k_cxlT","colab_type":"text"},"cell_type":"markdown","source":["## Classic ML"]},{"metadata":{"id":"-ZHyykUHcxlV","colab_type":"text"},"cell_type":"markdown","source":["we will use the following functions in order to use classical models and compare between them:  \n","(we use flatten in order to convert each X hours with shape (X,7) to 1 dim with 7X elements  \n"," we use classic_ml to fit and predict classic ml.  \n"," we use ml_by_history to return the best model trained by classic_ml and plot summary)"]},{"metadata":{"id":"f_XDA9IScxlY","colab_type":"code","colab":{}},"cell_type":"code","source":["def flatten(data):\n","    flatten_data = []\n","    for el in data:\n","        flatten_data.append(el.ravel())\n","    return flatten_data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EMQ8CvLscxla","colab_type":"code","colab":{}},"cell_type":"code","source":["from copy import deepcopy\n","def classic_ml(clf, n_in):\n","    train_X,train_Y = to_supervised(train_normalized,n_in)\n","    validation_X,validation_Y = to_supervised(validation_normalized,n_in)\n","    flatten_validation_x = flatten(validation_X)\n","    flatten_train_x = flatten(train_X) \n","    # training\n","    clf = clf.fit(flatten_train_x,train_Y)\n","    # prediction\n","    pred = clf.predict(flatten_validation_x)\n","    root_mse = rmse(validation_Y,pred)\n","    return clf, root_mse\n","\n","def ml_by_history(n_in_lst,regressor):\n","    min = float('inf')\n","    res = []\n","    modle_ml = None\n","    q = -1\n","    for n_in in n_in_lst:\n","        m,score = classic_ml(regressor, n_in)\n","        res.append(score)\n","        if min > score:\n","            modle_ml = deepcopy(m)\n","            min = score\n","            q = n_in\n","    plot(n_in_lst,res)\n","    plt.xlabel('n_in')\n","    plt.ylabel('rmse')\n","    plt.title('rmse as function of number of history hours')\n","    print('\\n*** BEST RESULT WITH {} HISTORY hours WITH RMSE OF: {} ***'.format(q,min))\n","    return min,modle_ml"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tgZLIkLzcxlo","colab_type":"text"},"cell_type":"markdown","source":["we will train linearegression:"]},{"metadata":{"id":"FlTec9dLcxlo","colab_type":"code","outputId":"8301e8a8-b90c-48eb-e7f3-fd1bb02b66d9","colab":{}},"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","rgr = LinearRegression()\n","rgr"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"]},"metadata":{"tags":[]},"execution_count":90}]},{"metadata":{"id":"YOX_JMl5cxlr","colab_type":"text"},"cell_type":"markdown","source":["this model is very simple and there are not many parameters, hence we dont have to do any tuning with the model parameters, but we will try to check with how many history hours we can get the most accurate and generalize model, in the following function we will check for the best model with history hours between 1 to 48:"]},{"metadata":{"scrolled":true,"id":"kl2CbcVhcxls","colab_type":"code","outputId":"1e697b8a-ac95-49dd-d046-110e65d6d1f0","colab":{}},"cell_type":"code","source":["min_rmse,model = ml_by_history([i for i in range(1,48)],rgr)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","*** BEST RESULT WITH 47 HISTORY hours WITH RMSE OF: 0.07923576894493851 ***\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZIAAAEXCAYAAACH/8KRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni4JERES6RUEiIiLd8v8AJHIKdFD/Io4AAAAASUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x1fe89ba1c88>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"6Oe12r5Xcxlv","colab_type":"text"},"cell_type":"markdown","source":["we have got great result on the validation set, but the validation set is depend on the n_in parameter, let's try to calculate the rmse on the test set:"]},{"metadata":{"id":"p9LyQOb7cxlw","colab_type":"code","outputId":"4cdacb2b-8da7-4e77-c685-99738ef638a1","colab":{}},"cell_type":"code","source":["n_in = 47\n","X,Y = to_supervised(test_data_normalized,n_in)\n","pred = model.predict(flatten(X))\n","rmse(Y,pred)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.11974072078110307"]},"metadata":{"tags":[]},"execution_count":137}]},{"metadata":{"id":"q8X-4Fdvcxly","colab_type":"text"},"cell_type":"markdown","source":["the result on the test set is almost the same as in the validation set.  \n","and *** we have got a new benchmark of ~0.1197 RMSE ***"]},{"metadata":{"id":"kgf9UQfccxlz","colab_type":"text"},"cell_type":"markdown","source":["let's try the same process on different model, now will will try to use LassoLars:"]},{"metadata":{"id":"P3nN8Xbmcxlz","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn import linear_model\n","reg = linear_model.LassoLars()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PvZj2-xEcxl1","colab_type":"code","outputId":"3ad7d947-406a-4101-8646-d1f1f4391e3e","colab":{}},"cell_type":"code","source":["min_rmse_lasso,model_lasso = ml_by_history([i for i in range(1,48)],reg)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","*** BEST RESULT WITH 33 HISTORY hours WITH RMSE OF: 0.12853364255653654 ***\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZ4AAAEXCAYAAACdwyIfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRFniMcYYE1H/H6BlPVmrF3RTAAAAAElFTkSuQmCC\n","text/plain":["<matplotlib.figure.Figure at 0x1fe89ba1358>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"Y5WBjIzmcxl4","colab_type":"code","outputId":"09c2bbf1-21d3-4cf0-dcf7-fe8c3f8b97ff","colab":{}},"cell_type":"code","source":["n_in = 33\n","X,Y = to_supervised(test_data_normalized, n_in)\n","pred = model_lasso.predict(flatten(X))\n","rmse(Y,pred)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.12226921931275868"]},"metadata":{"tags":[]},"execution_count":140}]},{"metadata":{"id":"CFOEEgWBcxl8","colab_type":"text"},"cell_type":"markdown","source":["the result we have got for Lasso model is not better than the result we have got for LinearRegression."]},{"metadata":{"id":"cruZMYYHcxl9","colab_type":"text"},"cell_type":"markdown","source":["## FORM NN"]},{"metadata":{"id":"_cMqSfhjcxl9","colab_type":"text"},"cell_type":"markdown","source":["import relevant packages:"]},{"metadata":{"id":"SGBEl_A7cxl_","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.layers import Dense,LSTM,Input,Dropout,Conv1D,Flatten,Activation\n","from keras.models import Model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h4rse6nscxmA","colab_type":"text"},"cell_type":"markdown","source":["this function will return list of callbacks to use:"]},{"metadata":{"id":"rs87B6VJcxmB","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.callbacks import *\n","import os\n","def set_callbacks(description='normal', path = './', patience=3):\n","    path += description + '/'\n","    try:\n","        os.mkdir(path)\n","    except FileExistsError:\n","        None\n","    cp = ModelCheckpoint(path + 'weights.{epoch:02d}-{val_loss:.2f}_'+ description + '.hdf5',save_best_only=True)\n","    #cp = ModelCheckpoint(path + 'best_weights.hdf5',save_best_only=True)\n","    es = EarlyStopping(patience=patience,monitor='loss')   \n","    log = CSVLogger(path + 'train_log.csv')\n","    tb = TensorBoard(log_dir=path + 'logs/')\n","    reduce_lr_loss = ReduceLROnPlateau(monitor='loss',factor=0.1, patience=2, verbose=1, epsilon=1e-3)\n","    cb = [reduce_lr_loss,cp,es]\n","    return cb"],"execution_count":0,"outputs":[]},{"metadata":{"id":"71zvVRBScxmE","colab_type":"text"},"cell_type":"markdown","source":["our first model will be very simple, we will use only one hidden layer. because this is time series data, we chose to use lstm layer as our hidden layer with 30 units."]},{"metadata":{"id":"pcifvid6cxmE","colab_type":"code","colab":{}},"cell_type":"code","source":["def build_nn_model(n_in,n_out,n_vars,units=30):\n","    inp = Input(shape=(n_in,n_vars))\n","    x = LSTM(units)(inp)\n","    output = Dense(n_out)(x)\n","    model = Model(inp,output)\n","    model.compile(loss='mean_squared_error',optimizer='adam')\n","    model.summary()\n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2sLaa8h1cxmH","colab_type":"text"},"cell_type":"markdown","source":["here we also use 47 hours history as we used in the classic model. (because it gave us the best result there)  \n","with 10 epochs and batch size of 32."]},{"metadata":{"scrolled":true,"id":"_vlbkLoicxmI","colab_type":"code","outputId":"f87e90b4-9ac5-4cef-b89f-2b3813506299","colab":{}},"cell_type":"code","source":["n_in=47\n","n_out=1\n","n_vars=train_normalized.shape[1]\n","\n","# series to supervised\n","validation_X,validation_Y = to_supervised(validation_normalized,n_in)\n","train_X,train_Y = to_supervised(train_normalized,n_in)\n","\n","# build NN model\n","model = build_nn_model(n_in,n_out,n_vars)\n","\n","# fit the model\n","history = model.fit(np.asarray(train_X),np.asarray(train_Y),validation_data=[np.asarray(validation_X),np.asarray(validation_Y)],\n","                    callbacks = set_callbacks(patience=5, path='./models_weights/' , description='_first1'),epochs=10,batch_size=32) \n","\n","# prediction & RMSE\n","nn_pred = model.predict(np.asarray(validation_X))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_8 (InputLayer)         (None, 47, 7)             0         \n","_________________________________________________________________\n","lstm_8 (LSTM)                (None, 30)                4560      \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 1)                 31        \n","=================================================================\n","Total params: 4,591\n","Trainable params: 4,591\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21615 samples, validate on 5368 samples\n","Epoch 1/10\n","21615/21615 [==============================] - 28s 1ms/step - loss: 0.0135 - val_loss: 0.0080\n","Epoch 2/10\n","21615/21615 [==============================] - 25s 1ms/step - loss: 0.0098 - val_loss: 0.0069\n","Epoch 3/10\n","21615/21615 [==============================] - 25s 1ms/step - loss: 0.0092 - val_loss: 0.0066\n","Epoch 4/10\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0088 - val_loss: 0.0066\n","Epoch 5/10\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0086 - val_loss: 0.0064\n","Epoch 6/10\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0084 - val_loss: 0.0062\n","Epoch 7/10\n","21600/21615 [============================>.] - ETA: 0s - loss: 0.0083\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0083 - val_loss: 0.0061\n","Epoch 8/10\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0079 - val_loss: 0.0059\n","Epoch 9/10\n","21600/21615 [============================>.] - ETA: 0s - loss: 0.0078\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0078 - val_loss: 0.0059\n","Epoch 10/10\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0078 - val_loss: 0.0059\n"],"name":"stdout"}]},{"metadata":{"id":"fkWgadVccxmK","colab_type":"text"},"cell_type":"markdown","source":["save the history, so we can later plot the loss:"]},{"metadata":{"id":"ivXorUqhcxmK","colab_type":"code","colab":{}},"cell_type":"code","source":["save_obj(history.history,'./models_weights/_first1/history')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cAYsQDZzcxmM","colab_type":"text"},"cell_type":"markdown","source":["this function will get an history and print the loss of the validation and train sets:"]},{"metadata":{"id":"WDgSszGncxmO","colab_type":"code","colab":{}},"cell_type":"code","source":["def plot_loss(history):\n","    plot([i for i in range(len(history['loss']))],history['loss'])\n","    plot([i for i in range(len(history['val_loss']))],history['val_loss'])\n","    valid_set = mpatches.Patch(color='orange', label='validation')\n","    train_set = mpatches.Patch(color='blue', label='train')\n","    plt.legend(handles=[valid_set,train_set])\n","    plt.xlabel('n_in')\n","    plt.ylabel('rmse')\n","    plt.title('rmse as function of number of history days')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1652rZOOcxmQ","colab_type":"code","outputId":"6d4c53e5-d155-4b6d-a738-2a28ec8a3028","colab":{}},"cell_type":"code","source":["plot_loss(load_obj('./models_weights/_first1/history'))"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZIAAAEXCAYAAACH/8KRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPgAAAABJRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x1fe89b64898>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"J_v5xAhHcxmU","colab_type":"text"},"cell_type":"markdown","source":["*** important facts about the plot ***\n","* the model has 4,591 parameters, and 21,623 samples to train on.\n","* we can see that as the epoch advanced the train loss decrease, the same happend in the validation set (except epoch 3).\n","* the loss of the validation set is lower than the train set since the first epoch, and as the number of epoch increase the train set loss decrease, and the vlidation set decrease as well.  \n","* we have got an intersting case , which the model fit better to the validation than to the training set, it can happen sometimes and it is not planned, but because we can notice that while the train set fit better, the validation loss decrease and because of the explanation below this case doesn't bother us."]},{"metadata":{"id":"P2snG5tTcxmV","colab_type":"text"},"cell_type":"markdown","source":["*** let's take a look where we had bad prediction: ***  \n","if we look at the input and output it's all numeric and this is very difficult for us to understand patterns in this way, hence we use the following function in order to display summary tables, the threshold parameter meaning is 'which error above the rmse we will consider as high enough to conclude in the summary':"]},{"metadata":{"id":"dRv6ktn-cxmV","colab_type":"code","colab":{}},"cell_type":"code","source":["def analyze_result(model,path,n_in,data = test_data_normalized,threshold = 0.2):\n","    X,Y = to_supervised(data,n_in)\n","    model.load_weights(path)\n","    score = rmse(Y,model.predict(np.asarray(X)))\n","    to_check = []\n","    for i,el in enumerate(X[0:-1]):\n","        if rmse(Y[i:i+1],model.predict(np.asarray(X[i:i+1]))) > score + threshold:\n","            to_check.append(i + n_in)\n","    counts_by_hour = pd.DataFrame(pd.Series(test_data_normalized.iloc[to_check,:].index.hour.values).value_counts()).reset_index()\n","    counts_by_hour.columns = ['hour','count']\n","    pm = counts_by_hour['hour'] > 12\n","    counts_by_ampm = pd.DataFrame([counts_by_hour[pm]['count'].sum(),counts_by_hour[[not i for i in pm]]['count'].sum()],['pm','am'])\n","    counts_by_day = pd.Series([calendar.day_name[my_date.weekday()] for my_date in test_data_normalized.iloc[to_check,:].index]).value_counts().reset_index()\n","    counts_by_day.columns = ['day','count']\n","    display(counts_by_hour)\n","    display(counts_by_ampm)\n","    display(counts_by_day)"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"JIg2OUn-cxmX","colab_type":"code","outputId":"186e9472-98e8-4d83-d2b8-e560d7befd34","colab":{}},"cell_type":"code","source":["n_in=47; path = './models_weights/_first1/best_weights.hdf5'\n","analyze_result(model,path,n_in)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hour</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>19</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>18</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>21</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>11</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>15</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>14</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>22</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>17</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>13</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>9</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>23</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>7</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    hour  count\n","0     19     10\n","1     20      7\n","2     18      7\n","3     12      6\n","4     10      5\n","5     21      4\n","6     11      4\n","7     15      3\n","8     14      3\n","9     22      2\n","10    17      2\n","11    13      2\n","12     9      2\n","13    23      1\n","14     8      1\n","15     7      1"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>pm</th>\n","      <td>41</td>\n","    </tr>\n","    <tr>\n","      <th>am</th>\n","      <td>19</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     0\n","pm  41\n","am  19"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>day</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Saturday</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Monday</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sunday</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Wednesday</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Thursday</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Tuesday</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Friday</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         day  count\n","0   Saturday     12\n","1     Monday     12\n","2     Sunday     11\n","3  Wednesday      9\n","4   Thursday      7\n","5    Tuesday      7\n","6     Friday      2"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"FmCqQWvBcxma","colab_type":"text"},"cell_type":"markdown","source":["we want to pay attention to the following facts:  \n","* in the summary only errors that get 0.2 above the rmse included. (60 errors)\n","* most of them occoured during the evening time. (41/60 => ~68%)\n","* a large number of the mistakes occoured during the days saturday and sunday. (~38%)  \n","  \n","  what can we infer from this ?  \n","Unfortunately, we do not know from which house the data is taken and how many people are living at home, and it make it difficult for us to understand the electricity consumption habits of the family members. by the facts we mentioned, most of the errors are during the evening, and in the weekend, so probably we can infer from it that during the day and in the middle of the week most of the time the family members at school, job and etc, so the consumption patterns in these hours are easy to learn. but at the evening and weekend time, they spend more time at home, use electricity often the the consumption pattern is more complex because of these reasons."]},{"metadata":{"id":"8Wg4Pa3Ecxmb","colab_type":"text"},"cell_type":"markdown","source":["let's see the RMSE on the validation set:"]},{"metadata":{"scrolled":true,"id":"OW6cAnUdcxmb","colab_type":"code","outputId":"db414a13-b95c-4249-8bf4-654c6a420e54","colab":{}},"cell_type":"code","source":["model.load_weights('./models_weights/_first1/best_weights.hdf5')\n","nn_pred = model.predict(np.asarray(validation_X))\n","rmse(validation_Y,nn_pred)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0765920460634663"]},"metadata":{"tags":[]},"execution_count":168}]},{"metadata":{"id":"VM57cUcgcxmd","colab_type":"text"},"cell_type":"markdown","source":["it looks like we have got better result with nn, but for this result we tuned with different batch_size and units (we run the model couple of times before this running) so the result depend on this parameters.  \n","let's check the rmse on the test set:"]},{"metadata":{"id":"gR_uxT83cxme","colab_type":"code","outputId":"ce46e1d7-6332-4b17-ca67-48fd5bab8a7f","colab":{}},"cell_type":"code","source":["X,Y = to_supervised(test_data_normalized,n_in)\n","rmse(Y,model.predict(np.asarray(X)))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.07969866897171837"]},"metadata":{"tags":[]},"execution_count":169}]},{"metadata":{"id":"iXlaomficxmh","colab_type":"text"},"cell_type":"markdown","source":["***we have got new benchmark, by improving the rmse to 0.0797 ***"]},{"metadata":{"id":"IIiAbIUUcxmi","colab_type":"text"},"cell_type":"markdown","source":["# * Explanation about the phenomenon that the train loss is higher than the validation loss *"]},{"metadata":{"id":"XMQKJQCScxmj","colab_type":"text"},"cell_type":"markdown","source":["while we trained this nn we were very surprising to see the the train loss is higher than the validation loss during all the training process.  \n","first thing we did is using 'Three fingers rule' in order to guess why this happens, we thought about the following reasons:  \n","** our split was not good enough ** - because probably the train set has more 'difficult' cases that the model has to learn, and the validation set doesn't consist them, so the validation consist only the easy cases that the model learn very easy, and hence the loss rate for the validation is lower.  \n","what we did?  \n","we thought maybe there is really difficult cases which can result from transition season, holidays, and many other reasons, and because the validation set cover only few months probably some of this cases doesn't appear there, so we tried to split the train and validation in a different way so the validation will cover one year. the reason for this split is that in the exploration we saw that over the years the use in electricity was similar. the result of this try, was the same, and the train loss was still higher than the validation loss during the train.  \n","** because the train set is bigger than the validation set the loss is higher ** - we calculate the loss by mse which means MEAN square error, so because it mean the square error this assumption is not logical.  \n","** the loss of the train set every epoch is calculated before the gradients have been applied while the validation loss calculated after the gradients have been applied **  \n","what we did?  \n","we decided to try to train the model and use our train set also for validation, if we will see that the validation loss is less than the train loss we can conclude that this assumption is right.  \n","the following snippet do this:\n"]},{"metadata":{"id":"dzfc4qDRcxmk","colab_type":"code","outputId":"6391cf2f-97c5-4584-bb3c-a42e566f7df6","colab":{}},"cell_type":"code","source":["n_in=47\n","n_out=1\n","n_vars=train_normalized.shape[1]\n","# series to supervised\n","validation_X,validation_Y = to_supervised(validation_normalized,n_in)\n","train_X,train_Y = to_supervised(train_normalized,n_in)\n","# build NN model\n","model = build_nn_model(n_in,n_out,n_vars)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_26 (InputLayer)        (None, 47, 7)             0         \n","_________________________________________________________________\n","lstm_31 (LSTM)               (None, 30)                4560      \n","_________________________________________________________________\n","dense_41 (Dense)             (None, 1)                 31        \n","=================================================================\n","Total params: 4,591\n","Trainable params: 4,591\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"iwFKFKFKcxmn","colab_type":"code","outputId":"ef0ef0f0-f600-4951-a739-4c73fcbe8cd6","colab":{}},"cell_type":"code","source":["# fit the model\n","history = model.fit(np.asarray(train_X),np.asarray(train_Y),validation_data=[np.asarray(train_X),np.asarray(train_Y)],\n","                    callbacks = set_callbacks(patience=5, path='./models_weights/' , description='_first1'),epochs=3,batch_size=32) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 21615 samples, validate on 21615 samples\n","Epoch 1/3\n","21615/21615 [==============================] - 34s 2ms/step - loss: 0.0128 - val_loss: 0.0098\n","Epoch 2/3\n","21615/21615 [==============================] - 28s 1ms/step - loss: 0.0095 - val_loss: 0.0093\n","Epoch 3/3\n","21615/21615 [==============================] - 27s 1ms/step - loss: 0.0090 - val_loss: 0.0087\n"],"name":"stdout"}]},{"metadata":{"id":"wjPTf-v8cxmp","colab_type":"text"},"cell_type":"markdown","source":["as we can see this assumption is right!! \n","*** and we learned something new :) ***  \n","conclusion about our data:  \n","probably usually when we use nn the data is more varied so the updated weights (after gradient) are not good enough to describe the validation data better than the train data before gradient applied, in our case the data is almost not varied, so after gradient applied the loss of the validation is lower than train loss before the weights updated."]},{"metadata":{"id":"Fh8hwggGcxmq","colab_type":"text"},"cell_type":"markdown","source":["we also found famouse tutorial by lasagne with mnist dataset, there same situation happend (https://lasagne.readthedocs.io/en/latest/user/tutorial.html)."]},{"metadata":{"id":"WBrAJ7Vwcxmq","colab_type":"text"},"cell_type":"markdown","source":["### Three Finger Rule (at least three...)  \n","so... how can we improve our nn model?  \n","* we can try to add more units to the LSTM layer or add new layers to increase the number of parameters, in order to fit the train data even more.  \n","* we can make our nn model deeper and see if it is helping (as we saw in lectures the deeper the layer, the more complex patterns it learns).  \n","* we can try to change n_in value, possibly with different value the model can generelize better.\n","* we can try feature engineering by adding new column for the 'active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.' as new feature as we saw in the exploration.\n","* we can try to use CNN layers in order to try to recognize the structure of data.  \n","* we can also try to change the batch_size. (we read some articles that using mini-batch can increase the generalization)  \n","* we can try to use feature extraction with cnn (as we saw in lecture, this will be good idea in case cnn will give us good result, so it recognize patterns in the data, and we can use this patterns as features)  \n","  \n","by these steps we can maybe fit the data better and recognize complex patterns in the data even during the evening and weekend time."]},{"metadata":{"id":"J5rviVy7cxmr","colab_type":"text"},"cell_type":"markdown","source":["### feature engineering  \n","first we will try feature engineering by adding new feature.  \n","'Sub_metering_4' that represents the active energy consumed every minute (in watt) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.  \n","the following function return train,validation sets with the new feature:"]},{"metadata":{"id":"Ly4KHP0qcxms","colab_type":"code","colab":{}},"cell_type":"code","source":["def add_sub_metering_4():\n","    #train_data = pd.read_csv('train_data.csv',sep=',', parse_dates=['Date'], infer_datetime_format=True, index_col = ['Date'])\n","    data = pd.read_csv('data_without_nan.csv',  parse_dates=['Date'], infer_datetime_format=True, index_col = ['Date'])\n","    data['Sub_metering_4'] = data['Global_active_power']*1000/60 - data['Sub_metering_1'] - data['Sub_metering_2'] - data['Sub_metering_3']\n","    data_by_days = data.resample('H').sum()\n","    split = 0.8\n","    q = round(split*len(data_by_days))\n","    _train = data_by_days[:q]\n","    _validation = data_by_days[q:]\n","    #normalization\n","    train = normalize(_train,_train)\n","    validation = normalize(_train,_validation)\n","    return train,validation"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6jMH7EGzcxmv","colab_type":"code","colab":{}},"cell_type":"code","source":["train_1_more_feature, validation_1_more_feature = add_sub_metering_4() "],"execution_count":0,"outputs":[]},{"metadata":{"id":"0jJLnPSecxmy","colab_type":"text"},"cell_type":"markdown","source":["we use the same nn as before, one hiddem layer of LSTM with 30 units: "]},{"metadata":{"scrolled":true,"id":"MUpBEpcGcxmy","colab_type":"code","outputId":"21317e12-e4ac-49f0-a2d4-475422e797bf","colab":{}},"cell_type":"code","source":["n_in=47\n","n_out=1\n","n_vars=train_1_more_feature.shape[1]\n","\n","# series to supervised\n","validation_X,validation_Y = to_supervised(validation_1_more_feature,n_in)\n","train_X,train_Y = to_supervised(train_1_more_feature,n_in)\n","\n","# build NN model\n","model = build_nn_model(n_in,n_out,n_vars)\n","\n","# fit the model\n","history = model.fit(np.asarray(train_X),np.asarray(train_Y),validation_data=[np.asarray(validation_X),np.asarray(validation_Y)],\n","                    callbacks = set_callbacks(patience=5, path='./models_weights/' , description='_feature_engineering'), batch_size = 32 ,epochs=15) \n","\n","# prediction & RMSE\n","nn_pred = model.predict(np.asarray(validation_X))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_9 (InputLayer)         (None, 47, 8)             0         \n","_________________________________________________________________\n","lstm_9 (LSTM)                (None, 30)                4680      \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 1)                 31        \n","=================================================================\n","Total params: 4,711\n","Trainable params: 4,711\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21615 samples, validate on 5368 samples\n","Epoch 1/15\n","21615/21615 [==============================] - 25s 1ms/step - loss: 0.0132 - val_loss: 0.0073\n","Epoch 2/15\n","21615/21615 [==============================] - 23s 1ms/step - loss: 0.0093 - val_loss: 0.0069\n","Epoch 3/15\n","21615/21615 [==============================] - 23s 1ms/step - loss: 0.0090 - val_loss: 0.0066\n","Epoch 4/15\n","21615/21615 [==============================] - 23s 1ms/step - loss: 0.0089 - val_loss: 0.0067\n","Epoch 5/15\n","21568/21615 [============================>.] - ETA: 0s - loss: 0.0086\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0086 - val_loss: 0.0063\n","Epoch 6/15\n","21615/21615 [==============================] - 23s 1ms/step - loss: 0.0082 - val_loss: 0.0061\n","Epoch 7/15\n","21615/21615 [==============================] - 23s 1ms/step - loss: 0.0082 - val_loss: 0.0061\n","Epoch 8/15\n","21615/21615 [==============================] - 23s 1ms/step - loss: 0.0081 - val_loss: 0.0061\n","Epoch 9/15\n","21568/21615 [============================>.] - ETA: 0s - loss: 0.0081\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21615/21615 [==============================] - 23s 1ms/step - loss: 0.0081 - val_loss: 0.0060\n","Epoch 10/15\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0080 - val_loss: 0.0060\n","Epoch 11/15\n","21600/21615 [============================>.] - ETA: 0s - loss: 0.0080- ETA: 0s - loss: 0.0\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0080 - val_loss: 0.0060\n","Epoch 12/15\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0080 - val_loss: 0.0060\n","Epoch 13/15\n","21568/21615 [============================>.] - ETA: 0s - loss: 0.0080\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0080 - val_loss: 0.0060\n","Epoch 14/15\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0080 - val_loss: 0.0060\n","Epoch 15/15\n","21568/21615 [============================>.] - ETA: 0s - loss: 0.0080\n","Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n","21615/21615 [==============================] - 23s 1ms/step - loss: 0.0080 - val_loss: 0.0060\n"],"name":"stdout"}]},{"metadata":{"id":"ocW8AfR1cxm0","colab_type":"text"},"cell_type":"markdown","source":["save the history:"]},{"metadata":{"id":"Lcl-GxOdcxm1","colab_type":"code","colab":{}},"cell_type":"code","source":["save_obj(history.history,'./models_weights/_feature_engineering/history')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yG6UduEpcxm3","colab_type":"code","outputId":"04cdaa76-4049-4237-b06f-cfd6f6fb3c13","colab":{}},"cell_type":"code","source":["plot_loss(history.history)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZIAAAEXCAYAAACH/8KRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4RO85MoAAAAASUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x1fe89c4da20>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"SnQaM6w7cxm5","colab_type":"text"},"cell_type":"markdown","source":["as we can see this plot is almost similar to the last plot, we can notice that from epoch 6, the loss is constant in both the train and validation sets. we can also notice that when the model fit better the train the validation loss decrease, hence if we success to fit the model even better than now we probably can get even better result. (because by this fact we can notice that there is not overfit although the train loss is low). "]},{"metadata":{"id":"xA1XLue0cxm5","colab_type":"text"},"cell_type":"markdown","source":["calculate RMSE on validation set:"]},{"metadata":{"scrolled":true,"id":"kTVARcFbcxm5","colab_type":"code","outputId":"587e9678-2f1c-4c75-f0f5-344f151f6a46","colab":{}},"cell_type":"code","source":["model.load_weights('./models_weights/_feature_engineering/best_weights.hdf5')\n","nn_pred = model.predict(np.asarray(validation_X))\n","rmse(validation_Y,nn_pred)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.07734512789782645"]},"metadata":{"tags":[]},"execution_count":177}]},{"metadata":{"id":"M4KnorWVcxm7","colab_type":"text"},"cell_type":"markdown","source":["we can notice that the result are not better than it was with 7 features (on the validation set).  \n","we can also see that the train loss is higher than it was, so we will try to fit this data set (with 8 features) with a little bit more parameters and see if there will be any improvement:"]},{"metadata":{"scrolled":true,"id":"Gb21-uQdcxm8","colab_type":"code","outputId":"46c2a1cd-6aae-4abc-c3d8-4efbaf73316f","colab":{}},"cell_type":"code","source":["n_in=47\n","n_out=1\n","n_vars=train_1_more_feature.shape[1]\n","\n","# series to supervised\n","validation_X,validation_Y = to_supervised(validation_1_more_feature,n_in)\n","train_X,train_Y = to_supervised(train_1_more_feature,n_in)\n","\n","# build NN model\n","model = build_nn_model(n_in,n_out,n_vars,50)\n","\n","# fit the model\n","history = model.fit(np.asarray(train_X),np.asarray(train_Y),validation_data=[np.asarray(validation_X),np.asarray(validation_Y)],\n","                    callbacks = set_callbacks(patience=5, path='./models_weights/' , description='_feature_engineering_more_units'), batch_size = 32 ,epochs=15) \n","\n","# prediction & RMSE\n","nn_pred = model.predict(np.asarray(validation_X))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_10 (InputLayer)        (None, 47, 8)             0         \n","_________________________________________________________________\n","lstm_10 (LSTM)               (None, 50)                11800     \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 1)                 51        \n","=================================================================\n","Total params: 11,851\n","Trainable params: 11,851\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21615 samples, validate on 5368 samples\n","Epoch 1/15\n","21615/21615 [==============================] - 27s 1ms/step - loss: 0.0111 - val_loss: 0.0069\n","Epoch 2/15\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0091 - val_loss: 0.0066\n","Epoch 3/15\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0087 - val_loss: 0.0067\n","Epoch 4/15\n","21615/21615 [==============================] - 25s 1ms/step - loss: 0.0085 - val_loss: 0.0063\n","Epoch 5/15\n","21600/21615 [============================>.] - ETA: 0s - loss: 0.0082\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21615/21615 [==============================] - 25s 1ms/step - loss: 0.0082 - val_loss: 0.0062\n","Epoch 6/15\n","21615/21615 [==============================] - 25s 1ms/step - loss: 0.0078 - val_loss: 0.0059\n","Epoch 7/15\n","21615/21615 [==============================] - 25s 1ms/step - loss: 0.0078 - val_loss: 0.0059\n","Epoch 8/15\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 9/15\n","21600/21615 [============================>.] - ETA: 0s - loss: 0.0077\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 10/15\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 11/15\n","21568/21615 [============================>.] - ETA: 0s - loss: 0.0077- ETA: 0s - \n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21615/21615 [==============================] - 25s 1ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 12/15\n","21615/21615 [==============================] - 26s 1ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 13/15\n","21600/21615 [============================>.] - ETA: 0s - loss: 0.0077\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21615/21615 [==============================] - 25s 1ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 14/15\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 15/15\n","21600/21615 [============================>.] - ETA: 0s - loss: 0.0077\n","Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n","21615/21615 [==============================] - 24s 1ms/step - loss: 0.0077 - val_loss: 0.0058\n"],"name":"stdout"}]},{"metadata":{"id":"H66eVQtMcxm9","colab_type":"code","colab":{}},"cell_type":"code","source":["save_obj(history.history,'./models_weights/_feature_engineering_more_units/history')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hyc-kAWLcxnB","colab_type":"code","outputId":"3e23b4e9-3ad7-475f-a9ad-a45adc37ab16","colab":{}},"cell_type":"code","source":["plot_loss(load_obj('./models_weights/_feature_engineering_more_units/history'))"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZIAAAEXCAYAAACH/8KRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x2625b5dff60>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"0WrObSUWcxnE","colab_type":"text"},"cell_type":"markdown","source":["there is nothing speecial to notice about this plot, just to see that until epoch 5 the train and validation sets loss decreses fast and later it was moderatly until constant. "]},{"metadata":{"id":"wyfqd-5FcxnF","colab_type":"code","outputId":"016ce4a6-23e4-4260-a16d-23ea3035d237","colab":{}},"cell_type":"code","source":["model.load_weights('./models_weights/_feature_engineering_more_units/best_weights.hdf5')\n","nn_pred = model.predict(np.asarray(validation_X))\n","rmse(validation_Y,nn_pred)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.07597617785683672"]},"metadata":{"tags":[]},"execution_count":183}]},{"metadata":{"id":"aGWnGZ5ocxnI","colab_type":"text"},"cell_type":"markdown","source":["it seems that we improved our last benchmark, to get this result we train our model with both 16 and 32 batch size, and at the end we decided 32 is better, because we have got better result, but this result depend on batch size parameter, let's check it on the test set:"]},{"metadata":{"id":"x7_SQQwBcxnI","colab_type":"code","colab":{}},"cell_type":"code","source":["test1 = pd.read_csv('test_data.csv',sep=',', parse_dates=['Date'], infer_datetime_format=True, index_col = ['Date'])\n","fill_missing_values(test1)\n","train_2 = pd.DataFrame.copy(train)\n","train_2['Sub_metering_4'] = train_2['Global_active_power']*1000/60 - train_2['Sub_metering_1'] - train_2['Sub_metering_2'] - train_2['Sub_metering_3']\n","test1['Sub_metering_4'] = test1['Global_active_power']*1000/60 - test1['Sub_metering_1'] - test1['Sub_metering_2'] - test1['Sub_metering_3']\n","test1_by_hours = test1.resample('H').sum()\n","test1_n = normalize(train_2,test1_by_hours)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3tVJlt6KcxnK","colab_type":"code","outputId":"4c812eb4-9361-4634-92e4-5ae295efb295","colab":{}},"cell_type":"code","source":["test1_n.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Global_active_power</th>\n","      <th>Global_reactive_power</th>\n","      <th>Voltage</th>\n","      <th>Global_intensity</th>\n","      <th>Sub_metering_1</th>\n","      <th>Sub_metering_2</th>\n","      <th>Sub_metering_3</th>\n","      <th>Sub_metering_4</th>\n","    </tr>\n","    <tr>\n","      <th>Date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2010-01-18 00:00:00</th>\n","      <td>0.203572</td>\n","      <td>0.176557</td>\n","      <td>0.924190</td>\n","      <td>0.191774</td>\n","      <td>0.0</td>\n","      <td>0.011486</td>\n","      <td>0.936205</td>\n","      <td>0.049779</td>\n","    </tr>\n","    <tr>\n","      <th>2010-01-18 01:00:00</th>\n","      <td>0.177912</td>\n","      <td>0.061240</td>\n","      <td>0.920817</td>\n","      <td>0.167623</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.842585</td>\n","      <td>0.044483</td>\n","    </tr>\n","    <tr>\n","      <th>2010-01-18 02:00:00</th>\n","      <td>0.025858</td>\n","      <td>0.061292</td>\n","      <td>0.930022</td>\n","      <td>0.025466</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.033140</td>\n","      <td>0.044036</td>\n","    </tr>\n","    <tr>\n","      <th>2010-01-18 03:00:00</th>\n","      <td>0.034548</td>\n","      <td>0.197199</td>\n","      <td>0.941867</td>\n","      <td>0.035390</td>\n","      <td>0.0</td>\n","      <td>0.014358</td>\n","      <td>0.033969</td>\n","      <td>0.047958</td>\n","    </tr>\n","    <tr>\n","      <th>2010-01-18 04:00:00</th>\n","      <td>0.026526</td>\n","      <td>0.070981</td>\n","      <td>0.930182</td>\n","      <td>0.026064</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.033140</td>\n","      <td>0.045166</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     Global_active_power  Global_reactive_power   Voltage  \\\n","Date                                                                        \n","2010-01-18 00:00:00             0.203572               0.176557  0.924190   \n","2010-01-18 01:00:00             0.177912               0.061240  0.920817   \n","2010-01-18 02:00:00             0.025858               0.061292  0.930022   \n","2010-01-18 03:00:00             0.034548               0.197199  0.941867   \n","2010-01-18 04:00:00             0.026526               0.070981  0.930182   \n","\n","                     Global_intensity  Sub_metering_1  Sub_metering_2  \\\n","Date                                                                    \n","2010-01-18 00:00:00          0.191774             0.0        0.011486   \n","2010-01-18 01:00:00          0.167623             0.0        0.000000   \n","2010-01-18 02:00:00          0.025466             0.0        0.000000   \n","2010-01-18 03:00:00          0.035390             0.0        0.014358   \n","2010-01-18 04:00:00          0.026064             0.0        0.000000   \n","\n","                     Sub_metering_3  Sub_metering_4  \n","Date                                                 \n","2010-01-18 00:00:00        0.936205        0.049779  \n","2010-01-18 01:00:00        0.842585        0.044483  \n","2010-01-18 02:00:00        0.033140        0.044036  \n","2010-01-18 03:00:00        0.033969        0.047958  \n","2010-01-18 04:00:00        0.033140        0.045166  "]},"metadata":{"tags":[]},"execution_count":185}]},{"metadata":{"id":"uTrQ4cP-cxnN","colab_type":"code","outputId":"9090da13-96de-4576-af81-957507af95e8","colab":{}},"cell_type":"code","source":["X,Y = to_supervised(test1_n,n_in)\n","rmse(Y,model.predict(np.asarray(X)))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.07631254844544175"]},"metadata":{"tags":[]},"execution_count":188}]},{"metadata":{"id":"-L3YSmU8cxnP","colab_type":"text"},"cell_type":"markdown","source":["as we can see the rmse for the test set is higher than the rmse for the validation set, but still we have got a little improvement. *** so our new benchmark is 0.0763 *** we don't consider this improvement to the new feature because it is really negligibal, and we also try to train the data with 7 features with more units and got same result, so we will continue with 7 features."]},{"metadata":{"id":"PxLtsLtocxnQ","colab_type":"text"},"cell_type":"markdown","source":["*** let's understand the errors better ***  \n","the rmse lower than in the last benchmark, now we will use the same default threshold (0.2) and look at the results:"]},{"metadata":{"id":"8cf1xsAucxnQ","colab_type":"code","outputId":"11f969c5-14e3-4358-e04c-3b2b78f4bf8e","colab":{}},"cell_type":"code","source":["n_in=47; path = './models_weights/_feature_engineering_more_units/best_weights.hdf5'\n","analyze_result(model,path,n_in,test1_n)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hour</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>19</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>18</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>12</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>10</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>20</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>14</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>21</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>11</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>17</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>15</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>22</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>13</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>7</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>23</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>9</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>6</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    hour  count\n","0     19      8\n","1     18      7\n","2     12      7\n","3     10      5\n","4     20      4\n","5     14      3\n","6     21      3\n","7     11      3\n","8     17      2\n","9     15      2\n","10    22      2\n","11    13      2\n","12     7      2\n","13     8      1\n","14    23      1\n","15     9      1\n","16     6      1"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>pm</th>\n","      <td>34</td>\n","    </tr>\n","    <tr>\n","      <th>am</th>\n","      <td>20</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     0\n","pm  34\n","am  20"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>day</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Saturday</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Monday</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sunday</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Wednesday</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Thursday</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Tuesday</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Friday</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         day  count\n","0   Saturday     12\n","1     Monday     11\n","2     Sunday     10\n","3  Wednesday      9\n","4   Thursday      6\n","5    Tuesday      3\n","6     Friday      3"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"nfse2nDxcxnV","colab_type":"text"},"cell_type":"markdown","source":["* we have 54 errors above the threshold.\n","* 32 of them occoured during the day (pm), this is ~63%.\n","* 22 of them occoured during the weekend (saturday,sunday), ~40%"]},{"metadata":{"collapsed":true,"id":"R8IDVsT0cxnV","colab_type":"text"},"cell_type":"markdown","source":["## best n_in"]},{"metadata":{"id":"bxZJ8aCzcxnV","colab_type":"text"},"cell_type":"markdown","source":["we will try to fit our model with different n_in (amount of history days) and check with which value we get the best result:"]},{"metadata":{"collapsed":true,"id":"1c6ltgtGcxnW","colab_type":"code","colab":{}},"cell_type":"code","source":["def nn_by_history_in(n_in_lst,_path,_description):\n","    best_model = None\n","    min = float('inf')\n","    q = -1\n","    res = []\n","    n_out=1\n","    for n_in in n_in_lst:\n","        n_vars=validation_normalized.shape[1]\n","\n","        # series to supervised\n","        validation_X,validation_Y = to_supervised(validation_normalized,n_in)\n","        train_X,train_Y = to_supervised(train_normalized,n_in)\n","\n","        # build NN model\n","        model = build_nn_model_v1_more_units(n_in,n_out,n_vars)\n","\n","        # fit the model\n","        history = model.fit(np.asarray(train_X),np.asarray(train_Y),validation_data=[np.asarray(validation_X),np.asarray(validation_Y)],\n","                            callbacks = set_callbacks(patience=3, path=_path , description=_description + '_' + str(n_in)),epochs=14, batch_size=32) \n","\n","        # prediction & RMSE\n","        model.load_weights(_path + _description + '_' + str(n_in) + '/best_weights.hdf5')\n","        nn_pred = model.predict(np.asarray(validation_X))\n","        score = sqrt(mse(validation_Y,nn_pred))\n","        res.append(score)\n","        if min > score:\n","            min = score\n","            best_model = deepcopy(model)\n","            q = n_in\n","    print('\\n*** BEST RESULT WITH {} HISTORY HOURS WITH RMSE OF: {} ***'.format(q,min))\n","    plot(n_in_lst,res)\n","    plt.xlabel('n_in')\n","    plt.ylabel('rmse')\n","    plt.title('rmse as function of number of history hours')\n","    return res,model"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"KcrtpfdKcxnX","colab_type":"code","outputId":"4a891fd7-87f4-4df0-89ae-aaea4ae2a937","colab":{}},"cell_type":"code","source":["path = './models_weights/'\n","desc = '_best_n_in'\n","res, best_m = nn_by_history_in([i for i in range(1,96,3)],path,desc)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_89 (InputLayer)        (None, 1, 7)              0         \n","_________________________________________________________________\n","lstm_89 (LSTM)               (None, 50)                11600     \n","_________________________________________________________________\n","dense_89 (Dense)             (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21661 samples, validate on 5414 samples\n","Epoch 1/14\n","21661/21661 [==============================] - 17s 796us/step - loss: 0.0113 - val_loss: 0.0082\n","Epoch 2/14\n","21661/21661 [==============================] - 5s 214us/step - loss: 0.0100 - val_loss: 0.0077\n","Epoch 3/14\n","21661/21661 [==============================] - 5s 208us/step - loss: 0.0099 - val_loss: 0.0078\n","Epoch 4/14\n","21661/21661 [==============================] - 5s 212us/step - loss: 0.0099 - val_loss: 0.0077\n","Epoch 5/14\n","21568/21661 [============================>.] - ETA: 0s - loss: 0.0098\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21661/21661 [==============================] - 8s 389us/step - loss: 0.0098 - val_loss: 0.0076\n","Epoch 6/14\n","21661/21661 [==============================] - 5s 208us/step - loss: 0.0097 - val_loss: 0.0075\n","Epoch 7/14\n","21344/21661 [============================>.] - ETA: 0s - loss: 0.0097\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21661/21661 [==============================] - 4s 202us/step - loss: 0.0097 - val_loss: 0.0075\n","Epoch 8/14\n","21661/21661 [==============================] - 4s 201us/step - loss: 0.0097 - val_loss: 0.0075\n","Epoch 9/14\n","21504/21661 [============================>.] - ETA: 0s - loss: 0.0097\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21661/21661 [==============================] - 4s 198us/step - loss: 0.0097 - val_loss: 0.0075\n","Epoch 10/14\n","21661/21661 [==============================] - 4s 199us/step - loss: 0.0097 - val_loss: 0.0075\n","Epoch 11/14\n","21344/21661 [============================>.] - ETA: 0s - loss: 0.0097\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21661/21661 [==============================] - 4s 204us/step - loss: 0.0097 - val_loss: 0.0075\n","Epoch 12/14\n","21661/21661 [==============================] - 4s 198us/step - loss: 0.0097 - val_loss: 0.0075\n","Epoch 13/14\n","21376/21661 [============================>.] - ETA: 0s - loss: 0.0097\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n","21661/21661 [==============================] - 4s 199us/step - loss: 0.0097 - val_loss: 0.0075\n","Epoch 14/14\n","21661/21661 [==============================] - 4s 201us/step - loss: 0.0097 - val_loss: 0.0075\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_90 (InputLayer)        (None, 4, 7)              0         \n","_________________________________________________________________\n","lstm_90 (LSTM)               (None, 50)                11600     \n","_________________________________________________________________\n","dense_90 (Dense)             (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21658 samples, validate on 5411 samples\n","Epoch 1/14\n","21658/21658 [==============================] - 18s 843us/step - loss: 0.0117 - val_loss: 0.0077\n","Epoch 2/14\n","21658/21658 [==============================] - 6s 271us/step - loss: 0.0097 - val_loss: 0.0074\n","Epoch 3/14\n","21658/21658 [==============================] - 6s 270us/step - loss: 0.0093 - val_loss: 0.0071\n","Epoch 4/14\n","21658/21658 [==============================] - 6s 290us/step - loss: 0.0092 - val_loss: 0.0072\n","Epoch 5/14\n","21504/21658 [============================>.] - ETA: 0s - loss: 0.0091\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21658/21658 [==============================] - 10s 439us/step - loss: 0.0091 - val_loss: 0.0071\n","Epoch 6/14\n","21658/21658 [==============================] - 6s 273us/step - loss: 0.0089 - val_loss: 0.0069\n","Epoch 7/14\n","21536/21658 [============================>.] - ETA: 0s - loss: 0.0089\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21658/21658 [==============================] - 6s 271us/step - loss: 0.0089 - val_loss: 0.0069\n","Epoch 8/14\n","21658/21658 [==============================] - 6s 270us/step - loss: 0.0089 - val_loss: 0.0069\n","Epoch 9/14\n","21568/21658 [============================>.] - ETA: 0s - loss: 0.0089\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21658/21658 [==============================] - 6s 268us/step - loss: 0.0089 - val_loss: 0.0069\n","Epoch 10/14\n","21658/21658 [==============================] - 6s 274us/step - loss: 0.0089 - val_loss: 0.0069\n","Epoch 11/14\n","21568/21658 [============================>.] - ETA: 0s - loss: 0.0089\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21658/21658 [==============================] - 6s 271us/step - loss: 0.0089 - val_loss: 0.0069\n","Epoch 12/14\n","21658/21658 [==============================] - 6s 271us/step - loss: 0.0089 - val_loss: 0.0069\n","Epoch 13/14\n","21600/21658 [============================>.] - ETA: 0s - loss: 0.0089\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n","21658/21658 [==============================] - 6s 269us/step - loss: 0.0089 - val_loss: 0.0069\n","Epoch 14/14\n","21658/21658 [==============================] - 6s 265us/step - loss: 0.0089 - val_loss: 0.0069\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_91 (InputLayer)        (None, 7, 7)              0         \n","_________________________________________________________________\n","lstm_91 (LSTM)               (None, 50)                11600     \n","_________________________________________________________________\n","dense_91 (Dense)             (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21655 samples, validate on 5408 samples\n","Epoch 1/14\n","21655/21655 [==============================] - 20s 931us/step - loss: 0.0120 - val_loss: 0.0076\n","Epoch 2/14\n","21655/21655 [==============================] - 7s 333us/step - loss: 0.0096 - val_loss: 0.0071\n","Epoch 3/14\n","21655/21655 [==============================] - 8s 357us/step - loss: 0.0092 - val_loss: 0.0075\n","Epoch 4/14\n","21655/21655 [==============================] - 8s 349us/step - loss: 0.0091 - val_loss: 0.0069\n","Epoch 5/14\n","21504/21655 [============================>.] - ETA: 0s - loss: 0.0091\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21655/21655 [==============================] - 11s 513us/step - loss: 0.0090 - val_loss: 0.0069\n","Epoch 6/14\n","21655/21655 [==============================] - 8s 355us/step - loss: 0.0088 - val_loss: 0.0068\n","Epoch 7/14\n","21600/21655 [============================>.] - ETA: 0s - loss: 0.0088\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21655/21655 [==============================] - 8s 365us/step - loss: 0.0088 - val_loss: 0.0068\n","Epoch 8/14\n","21655/21655 [==============================] - 7s 346us/step - loss: 0.0088 - val_loss: 0.0067\n","Epoch 9/14\n","21600/21655 [============================>.] - ETA: 0s - loss: 0.0088\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21655/21655 [==============================] - 8s 354us/step - loss: 0.0088 - val_loss: 0.0067\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 10/14\n","21655/21655 [==============================] - 7s 333us/step - loss: 0.0088 - val_loss: 0.0067\n","Epoch 11/14\n","21600/21655 [============================>.] - ETA: 0s - loss: 0.0088\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21655/21655 [==============================] - 7s 318us/step - loss: 0.0088 - val_loss: 0.0067\n","Epoch 12/14\n","21655/21655 [==============================] - 7s 335us/step - loss: 0.0088 - val_loss: 0.0067\n","Epoch 13/14\n","21536/21655 [============================>.] - ETA: 0s - loss: 0.0088\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n","21655/21655 [==============================] - 7s 332us/step - loss: 0.0088 - val_loss: 0.0067\n","Epoch 14/14\n","21655/21655 [==============================] - 8s 348us/step - loss: 0.0088 - val_loss: 0.0067\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_92 (InputLayer)        (None, 10, 7)             0         \n","_________________________________________________________________\n","lstm_92 (LSTM)               (None, 50)                11600     \n","_________________________________________________________________\n","dense_92 (Dense)             (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21652 samples, validate on 5405 samples\n","Epoch 1/14\n","21652/21652 [==============================] - 31s 1ms/step - loss: 0.0120 - val_loss: 0.0073\n","Epoch 2/14\n","21652/21652 [==============================] - 8s 390us/step - loss: 0.0095 - val_loss: 0.0069\n","Epoch 3/14\n","21652/21652 [==============================] - 8s 390us/step - loss: 0.0091 - val_loss: 0.0068\n","Epoch 4/14\n","21652/21652 [==============================] - 8s 391us/step - loss: 0.0089 - val_loss: 0.0065\n","Epoch 5/14\n","21504/21652 [============================>.] - ETA: 0s - loss: 0.0088\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21652/21652 [==============================] - 12s 564us/step - loss: 0.0088 - val_loss: 0.0068\n","Epoch 6/14\n","21652/21652 [==============================] - 9s 395us/step - loss: 0.0085 - val_loss: 0.0065\n","Epoch 7/14\n","21652/21652 [==============================] - 9s 393us/step - loss: 0.0085 - val_loss: 0.0064\n","Epoch 8/14\n","21652/21652 [==============================] - 8s 392us/step - loss: 0.0084 - val_loss: 0.0064\n","Epoch 9/14\n","21652/21652 [==============================] - 8s 388us/step - loss: 0.0084 - val_loss: 0.0064\n","Epoch 10/14\n","21568/21652 [============================>.] - ETA: 0s - loss: 0.0084\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21652/21652 [==============================] - 9s 412us/step - loss: 0.0084 - val_loss: 0.0064\n","Epoch 11/14\n","21652/21652 [==============================] - 9s 407us/step - loss: 0.0084 - val_loss: 0.0064\n","Epoch 12/14\n","21632/21652 [============================>.] - ETA: 0s - loss: 0.0084\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21652/21652 [==============================] - 9s 404us/step - loss: 0.0084 - val_loss: 0.0064\n","Epoch 13/14\n","21652/21652 [==============================] - 9s 408us/step - loss: 0.0084 - val_loss: 0.0064\n","Epoch 14/14\n","21632/21652 [============================>.] - ETA: 0s - loss: 0.0084\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21652/21652 [==============================] - 9s 401us/step - loss: 0.0084 - val_loss: 0.0064\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_93 (InputLayer)        (None, 13, 7)             0         \n","_________________________________________________________________\n","lstm_93 (LSTM)               (None, 50)                11600     \n","_________________________________________________________________\n","dense_93 (Dense)             (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21649 samples, validate on 5402 samples\n","Epoch 1/14\n","21649/21649 [==============================] - 23s 1ms/step - loss: 0.0115 - val_loss: 0.0073\n","Epoch 2/14\n","21649/21649 [==============================] - 10s 460us/step - loss: 0.0093 - val_loss: 0.0070\n","Epoch 3/14\n","21649/21649 [==============================] - 10s 458us/step - loss: 0.0089 - val_loss: 0.0068\n","Epoch 4/14\n","21649/21649 [==============================] - 10s 457us/step - loss: 0.0087 - val_loss: 0.0065\n","Epoch 5/14\n","21568/21649 [============================>.] - ETA: 0s - loss: 0.0086\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21649/21649 [==============================] - 14s 629us/step - loss: 0.0086 - val_loss: 0.0064\n","Epoch 6/14\n","21649/21649 [==============================] - 10s 457us/step - loss: 0.0084 - val_loss: 0.0063\n","Epoch 7/14\n","21632/21649 [============================>.] - ETA: 0s - loss: 0.0083\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21649/21649 [==============================] - 10s 459us/step - loss: 0.0083 - val_loss: 0.0063\n","Epoch 8/14\n","21649/21649 [==============================] - 10s 464us/step - loss: 0.0083 - val_loss: 0.0062\n","Epoch 9/14\n","21632/21649 [============================>.] - ETA: 0s - loss: 0.0083\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21649/21649 [==============================] - 10s 475us/step - loss: 0.0083 - val_loss: 0.0062\n","Epoch 10/14\n","21649/21649 [==============================] - 10s 465us/step - loss: 0.0083 - val_loss: 0.0062\n","Epoch 11/14\n","21649/21649 [==============================] - 10s 470us/step - loss: 0.0083 - val_loss: 0.0062\n","Epoch 12/14\n","21649/21649 [==============================] - 10s 458us/step - loss: 0.0083 - val_loss: 0.0062\n","Epoch 13/14\n","21568/21649 [============================>.] - ETA: 0s - loss: 0.0083\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21649/21649 [==============================] - 10s 453us/step - loss: 0.0083 - val_loss: 0.0062\n","Epoch 14/14\n","21649/21649 [==============================] - 10s 459us/step - loss: 0.0083 - val_loss: 0.0062\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_94 (InputLayer)        (None, 16, 7)             0         \n","_________________________________________________________________\n","lstm_94 (LSTM)               (None, 50)                11600     \n","_________________________________________________________________\n","dense_94 (Dense)             (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21646 samples, validate on 5399 samples\n","Epoch 1/14\n","21646/21646 [==============================] - 24s 1ms/step - loss: 0.0125 - val_loss: 0.0074\n","Epoch 2/14\n","21646/21646 [==============================] - 12s 533us/step - loss: 0.0095 - val_loss: 0.0073\n","Epoch 3/14\n","21646/21646 [==============================] - 11s 528us/step - loss: 0.0089 - val_loss: 0.0066\n","Epoch 4/14\n","21646/21646 [==============================] - 11s 520us/step - loss: 0.0087 - val_loss: 0.0065\n","Epoch 5/14\n","21632/21646 [============================>.] - ETA: 0s - loss: 0.0086\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21646/21646 [==============================] - 15s 713us/step - loss: 0.0086 - val_loss: 0.0065\n","Epoch 6/14\n","21646/21646 [==============================] - 12s 544us/step - loss: 0.0083 - val_loss: 0.0062\n","Epoch 7/14\n","21646/21646 [==============================] - 12s 550us/step - loss: 0.0082 - val_loss: 0.0062\n","Epoch 8/14\n","21646/21646 [==============================] - 12s 557us/step - loss: 0.0082 - val_loss: 0.0061\n","Epoch 9/14\n","21600/21646 [============================>.] - ETA: 0s - loss: 0.0082\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21646/21646 [==============================] - 12s 558us/step - loss: 0.0082 - val_loss: 0.0061\n","Epoch 10/14\n","21646/21646 [==============================] - 12s 541us/step - loss: 0.0082 - val_loss: 0.0061\n","Epoch 11/14\n","21568/21646 [============================>.] - ETA: 0s - loss: 0.0081\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21646/21646 [==============================] - 12s 548us/step - loss: 0.0082 - val_loss: 0.0061\n","Epoch 12/14\n","21646/21646 [==============================] - 12s 546us/step - loss: 0.0081 - val_loss: 0.0061\n","Epoch 13/14\n","21536/21646 [============================>.] - ETA: 0s - loss: 0.0081\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21646/21646 [==============================] - 12s 544us/step - loss: 0.0081 - val_loss: 0.0061\n","Epoch 14/14\n","21646/21646 [==============================] - 12s 543us/step - loss: 0.0081 - val_loss: 0.0061\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_95 (InputLayer)        (None, 19, 7)             0         \n","_________________________________________________________________\n","lstm_95 (LSTM)               (None, 50)                11600     \n","_________________________________________________________________\n","dense_95 (Dense)             (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21643 samples, validate on 5396 samples\n","Epoch 1/14\n","21643/21643 [==============================] - 27s 1ms/step - loss: 0.0119 - val_loss: 0.0073\n","Epoch 2/14\n","21643/21643 [==============================] - 13s 616us/step - loss: 0.0093 - val_loss: 0.0067\n","Epoch 3/14\n","21643/21643 [==============================] - 13s 596us/step - loss: 0.0088 - val_loss: 0.0065\n","Epoch 4/14\n","21643/21643 [==============================] - 13s 605us/step - loss: 0.0086 - val_loss: 0.0066\n","Epoch 5/14\n","21600/21643 [============================>.] - ETA: 0s - loss: 0.0085\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21643/21643 [==============================] - 18s 829us/step - loss: 0.0085 - val_loss: 0.0063\n","Epoch 6/14\n","21643/21643 [==============================] - 13s 618us/step - loss: 0.0082 - val_loss: 0.0061\n","Epoch 7/14\n","21643/21643 [==============================] - 13s 613us/step - loss: 0.0081 - val_loss: 0.0061\n","Epoch 8/14\n","21643/21643 [==============================] - 13s 615us/step - loss: 0.0081 - val_loss: 0.0061\n","Epoch 9/14\n","21600/21643 [============================>.] - ETA: 0s - loss: 0.0081\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21643/21643 [==============================] - 14s 624us/step - loss: 0.0081 - val_loss: 0.0061\n","Epoch 10/14\n","21643/21643 [==============================] - 13s 622us/step - loss: 0.0081 - val_loss: 0.0061\n","Epoch 11/14\n","21600/21643 [============================>.] - ETA: 0s - loss: 0.0081\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21643/21643 [==============================] - 13s 622us/step - loss: 0.0081 - val_loss: 0.0061\n","Epoch 12/14\n","21643/21643 [==============================] - 13s 603us/step - loss: 0.0080 - val_loss: 0.0061\n","Epoch 13/14\n","21568/21643 [============================>.] - ETA: 0s - loss: 0.0080\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21643/21643 [==============================] - 13s 602us/step - loss: 0.0080 - val_loss: 0.0061\n","Epoch 14/14\n","21643/21643 [==============================] - 13s 614us/step - loss: 0.0080 - val_loss: 0.0061\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_96 (InputLayer)        (None, 22, 7)             0         \n","_________________________________________________________________\n","lstm_96 (LSTM)               (None, 50)                11600     \n","_________________________________________________________________\n","dense_96 (Dense)             (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21640 samples, validate on 5393 samples\n","Epoch 1/14\n","21640/21640 [==============================] - 27s 1ms/step - loss: 0.0123 - val_loss: 0.0072\n","Epoch 2/14\n","21640/21640 [==============================] - 15s 670us/step - loss: 0.0093 - val_loss: 0.0067\n","Epoch 3/14\n","21640/21640 [==============================] - 15s 685us/step - loss: 0.0089 - val_loss: 0.0067\n","Epoch 4/14\n","21640/21640 [==============================] - 15s 701us/step - loss: 0.0087 - val_loss: 0.0064\n","Epoch 5/14\n","21600/21640 [============================>.] - ETA: 0s - loss: 0.0085\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21640/21640 [==============================] - 19s 858us/step - loss: 0.0085 - val_loss: 0.0064\n","Epoch 6/14\n","21640/21640 [==============================] - 15s 697us/step - loss: 0.0081 - val_loss: 0.0061\n","Epoch 7/14\n","21640/21640 [==============================] - 15s 670us/step - loss: 0.0081 - val_loss: 0.0061\n","Epoch 8/14\n","21640/21640 [==============================] - 15s 679us/step - loss: 0.0081 - val_loss: 0.0061\n","Epoch 9/14\n","21600/21640 [============================>.] - ETA: 0s - loss: 0.0081\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21640/21640 [==============================] - 15s 693us/step - loss: 0.0081 - val_loss: 0.0061\n","Epoch 10/14\n","21640/21640 [==============================] - 15s 684us/step - loss: 0.0080 - val_loss: 0.0061\n","Epoch 11/14\n","21568/21640 [============================>.] - ETA: 0s - loss: 0.0080\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21640/21640 [==============================] - 15s 682us/step - loss: 0.0080 - val_loss: 0.0061\n","Epoch 12/14\n","21640/21640 [==============================] - 15s 679us/step - loss: 0.0080 - val_loss: 0.0061\n","Epoch 13/14\n","21632/21640 [============================>.] - ETA: 0s - loss: 0.0080\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21640/21640 [==============================] - 14s 667us/step - loss: 0.0080 - val_loss: 0.0061\n","Epoch 14/14\n","21640/21640 [==============================] - 15s 679us/step - loss: 0.0080 - val_loss: 0.0061\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_97 (InputLayer)        (None, 25, 7)             0         \n","_________________________________________________________________\n","lstm_97 (LSTM)               (None, 50)                11600     \n","_________________________________________________________________\n","dense_97 (Dense)             (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21637 samples, validate on 5390 samples\n","Epoch 1/14\n","21637/21637 [==============================] - 30s 1ms/step - loss: 0.0119 - val_loss: 0.0074\n","Epoch 2/14\n","21637/21637 [==============================] - 16s 734us/step - loss: 0.0093 - val_loss: 0.0066\n","Epoch 3/14\n","21637/21637 [==============================] - 17s 764us/step - loss: 0.0088 - val_loss: 0.0065\n","Epoch 4/14\n"],"name":"stdout"},{"output_type":"stream","text":["21637/21637 [==============================] - 16s 755us/step - loss: 0.0086 - val_loss: 0.0062\n","Epoch 5/14\n","21600/21637 [============================>.] - ETA: 0s - loss: 0.0083\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21637/21637 [==============================] - 20s 935us/step - loss: 0.0083 - val_loss: 0.0063\n","Epoch 6/14\n","21637/21637 [==============================] - 17s 765us/step - loss: 0.0080 - val_loss: 0.0060\n","Epoch 7/14\n","21637/21637 [==============================] - 16s 762us/step - loss: 0.0080 - val_loss: 0.0060\n","Epoch 8/14\n","21637/21637 [==============================] - 16s 757us/step - loss: 0.0079 - val_loss: 0.0060\n","Epoch 9/14\n","21600/21637 [============================>.] - ETA: 0s - loss: 0.0079\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21637/21637 [==============================] - 16s 760us/step - loss: 0.0079 - val_loss: 0.0059\n","Epoch 10/14\n","21637/21637 [==============================] - 16s 760us/step - loss: 0.0079 - val_loss: 0.0059\n","Epoch 11/14\n","21568/21637 [============================>.] - ETA: 0s - loss: 0.0079\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21637/21637 [==============================] - 16s 738us/step - loss: 0.0079 - val_loss: 0.0059\n","Epoch 12/14\n","21637/21637 [==============================] - 16s 731us/step - loss: 0.0079 - val_loss: 0.0059\n","Epoch 13/14\n","21568/21637 [============================>.] - ETA: 0s - loss: 0.0079\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21637/21637 [==============================] - 16s 736us/step - loss: 0.0079 - val_loss: 0.0059\n","Epoch 14/14\n","21637/21637 [==============================] - 16s 761us/step - loss: 0.0079 - val_loss: 0.0059\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_98 (InputLayer)        (None, 28, 7)             0         \n","_________________________________________________________________\n","lstm_98 (LSTM)               (None, 50)                11600     \n","_________________________________________________________________\n","dense_98 (Dense)             (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21634 samples, validate on 5387 samples\n","Epoch 1/14\n","21634/21634 [==============================] - 31s 1ms/step - loss: 0.0112 - val_loss: 0.0078\n","Epoch 2/14\n","21634/21634 [==============================] - 20s 908us/step - loss: 0.0092 - val_loss: 0.0066\n","Epoch 3/14\n","21634/21634 [==============================] - 21s 959us/step - loss: 0.0087 - val_loss: 0.0067\n","Epoch 4/14\n","21634/21634 [==============================] - 21s 951us/step - loss: 0.0084 - val_loss: 0.0060\n","Epoch 5/14\n","21634/21634 [==============================] - 20s 911us/step - loss: 0.0082 - val_loss: 0.0060\n","Epoch 6/14\n","21634/21634 [==============================] - 20s 906us/step - loss: 0.0081 - val_loss: 0.0060\n","Epoch 7/14\n","21634/21634 [==============================] - 20s 920us/step - loss: 0.0081 - val_loss: 0.0066\n","Epoch 8/14\n","21600/21634 [============================>.] - ETA: 0s - loss: 0.0080\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21634/21634 [==============================] - 24s 1ms/step - loss: 0.0080 - val_loss: 0.0058\n","Epoch 9/14\n","21634/21634 [==============================] - 19s 864us/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 10/14\n","21600/21634 [============================>.] - ETA: 0s - loss: 0.0077\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21634/21634 [==============================] - 19s 867us/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 11/14\n","21634/21634 [==============================] - 19s 864us/step - loss: 0.0076 - val_loss: 0.0058\n","Epoch 12/14\n","21632/21634 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21634/21634 [==============================] - 19s 869us/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 13/14\n","21634/21634 [==============================] - 19s 870us/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 14/14\n","21632/21634 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21634/21634 [==============================] - 19s 891us/step - loss: 0.0076 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_99 (InputLayer)        (None, 31, 7)             0         \n","_________________________________________________________________\n","lstm_99 (LSTM)               (None, 50)                11600     \n","_________________________________________________________________\n","dense_99 (Dense)             (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21631 samples, validate on 5384 samples\n","Epoch 1/14\n","21631/21631 [==============================] - 37s 2ms/step - loss: 0.0117 - val_loss: 0.0076\n","Epoch 2/14\n","21631/21631 [==============================] - 23s 1ms/step - loss: 0.0093 - val_loss: 0.0078\n","Epoch 3/14\n","21631/21631 [==============================] - 23s 1ms/step - loss: 0.0088 - val_loss: 0.0064\n","Epoch 4/14\n","21631/21631 [==============================] - 25s 1ms/step - loss: 0.0083 - val_loss: 0.0060\n","Epoch 5/14\n","21631/21631 [==============================] - 26s 1ms/step - loss: 0.0082 - val_loss: 0.0061\n","Epoch 6/14\n","21631/21631 [==============================] - 27s 1ms/step - loss: 0.0081 - val_loss: 0.0062\n","Epoch 7/14\n","21631/21631 [==============================] - 27s 1ms/step - loss: 0.0080 - val_loss: 0.0063\n","Epoch 8/14\n","21568/21631 [============================>.] - ETA: 0s - loss: 0.0080\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21631/21631 [==============================] - 33s 2ms/step - loss: 0.0080 - val_loss: 0.0060\n","Epoch 9/14\n","21631/21631 [==============================] - 30s 1ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 10/14\n","21568/21631 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21631/21631 [==============================] - 30s 1ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 11/14\n","21631/21631 [==============================] - 34s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 12/14\n","21568/21631 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21631/21631 [==============================] - 32s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 13/14\n","21631/21631 [==============================] - 31s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 14/14\n","21568/21631 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21631/21631 [==============================] - 31s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_100 (InputLayer)       (None, 34, 7)             0         \n","_________________________________________________________________\n","lstm_100 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_100 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["Train on 21628 samples, validate on 5381 samples\n","Epoch 1/14\n","21628/21628 [==============================] - 37s 2ms/step - loss: 0.0132 - val_loss: 0.0076\n","Epoch 2/14\n","21628/21628 [==============================] - 22s 1ms/step - loss: 0.0096 - val_loss: 0.0069\n","Epoch 3/14\n","21628/21628 [==============================] - 22s 1ms/step - loss: 0.0089 - val_loss: 0.0064\n","Epoch 4/14\n","21628/21628 [==============================] - 22s 1ms/step - loss: 0.0086 - val_loss: 0.0064\n","Epoch 5/14\n","21628/21628 [==============================] - 21s 952us/step - loss: 0.0084 - val_loss: 0.0061\n","Epoch 6/14\n","21628/21628 [==============================] - 21s 967us/step - loss: 0.0083 - val_loss: 0.0060\n","Epoch 7/14\n","21628/21628 [==============================] - 21s 967us/step - loss: 0.0081 - val_loss: 0.0061\n","Epoch 8/14\n","21568/21628 [============================>.] - ETA: 0s - loss: 0.0081\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21628/21628 [==============================] - 26s 1ms/step - loss: 0.0080 - val_loss: 0.0058\n","Epoch 9/14\n","21628/21628 [==============================] - 22s 1ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 10/14\n","21600/21628 [============================>.] - ETA: 0s - loss: 0.0077\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21628/21628 [==============================] - 22s 1ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 11/14\n","21628/21628 [==============================] - 21s 988us/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 12/14\n","21600/21628 [============================>.] - ETA: 0s - loss: 0.0077\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21628/21628 [==============================] - 20s 946us/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 13/14\n","21628/21628 [==============================] - 21s 954us/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 14/14\n","21568/21628 [============================>.] - ETA: 0s - loss: 0.0077\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21628/21628 [==============================] - 21s 967us/step - loss: 0.0077 - val_loss: 0.0058\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_101 (InputLayer)       (None, 37, 7)             0         \n","_________________________________________________________________\n","lstm_101 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_101 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21625 samples, validate on 5378 samples\n","Epoch 1/14\n","21625/21625 [==============================] - 37s 2ms/step - loss: 0.0122 - val_loss: 0.0072\n","Epoch 2/14\n","21625/21625 [==============================] - 22s 1ms/step - loss: 0.0092 - val_loss: 0.0067\n","Epoch 3/14\n","21625/21625 [==============================] - 23s 1ms/step - loss: 0.0088 - val_loss: 0.0063\n","Epoch 4/14\n","21625/21625 [==============================] - 22s 1ms/step - loss: 0.0085 - val_loss: 0.0062\n","Epoch 5/14\n","21625/21625 [==============================] - 23s 1ms/step - loss: 0.0082 - val_loss: 0.0060\n","Epoch 6/14\n","21625/21625 [==============================] - 23s 1ms/step - loss: 0.0081 - val_loss: 0.0058\n","Epoch 7/14\n","21625/21625 [==============================] - 22s 1ms/step - loss: 0.0080 - val_loss: 0.0059\n","Epoch 8/14\n","21568/21625 [============================>.] - ETA: 0s - loss: 0.0079\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21625/21625 [==============================] - 27s 1ms/step - loss: 0.0079 - val_loss: 0.0059\n","Epoch 9/14\n","21625/21625 [==============================] - 22s 1ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 10/14\n","21600/21625 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21625/21625 [==============================] - 22s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 11/14\n","21625/21625 [==============================] - 22s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 12/14\n","21600/21625 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21625/21625 [==============================] - 22s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 13/14\n","21625/21625 [==============================] - 23s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 14/14\n","21600/21625 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21625/21625 [==============================] - 24s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_102 (InputLayer)       (None, 40, 7)             0         \n","_________________________________________________________________\n","lstm_102 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_102 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21622 samples, validate on 5375 samples\n","Epoch 1/14\n","21622/21622 [==============================] - 38s 2ms/step - loss: 0.0115 - val_loss: 0.0086\n","Epoch 2/14\n","21622/21622 [==============================] - 25s 1ms/step - loss: 0.0092 - val_loss: 0.0068\n","Epoch 3/14\n","21622/21622 [==============================] - 25s 1ms/step - loss: 0.0088 - val_loss: 0.0068\n","Epoch 4/14\n","21622/21622 [==============================] - 25s 1ms/step - loss: 0.0085 - val_loss: 0.0065\n","Epoch 5/14\n","21622/21622 [==============================] - 25s 1ms/step - loss: 0.0082 - val_loss: 0.0061\n","Epoch 6/14\n","21622/21622 [==============================] - 25s 1ms/step - loss: 0.0081 - val_loss: 0.0060\n","Epoch 7/14\n","21622/21622 [==============================] - 25s 1ms/step - loss: 0.0080 - val_loss: 0.0061\n","Epoch 8/14\n","21600/21622 [============================>.] - ETA: 0s - loss: 0.0079\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21622/21622 [==============================] - 29s 1ms/step - loss: 0.0079 - val_loss: 0.0063\n","Epoch 9/14\n","21622/21622 [==============================] - 25s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 10/14\n","21568/21622 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21622/21622 [==============================] - 25s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 11/14\n","21622/21622 [==============================] - 25s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 12/14\n","21568/21622 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21622/21622 [==============================] - 25s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 13/14\n","21622/21622 [==============================] - 25s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 14/14\n","21568/21622 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21622/21622 [==============================] - 25s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_103 (InputLayer)       (None, 43, 7)             0         \n","_________________________________________________________________\n","lstm_103 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_103 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["Train on 21619 samples, validate on 5372 samples\n","Epoch 1/14\n","21619/21619 [==============================] - 40s 2ms/step - loss: 0.0118 - val_loss: 0.0072\n","Epoch 2/14\n","21619/21619 [==============================] - 27s 1ms/step - loss: 0.0092 - val_loss: 0.0064\n","Epoch 3/14\n","21619/21619 [==============================] - 27s 1ms/step - loss: 0.0086 - val_loss: 0.0062\n","Epoch 4/14\n","21619/21619 [==============================] - 26s 1ms/step - loss: 0.0083 - val_loss: 0.0062\n","Epoch 5/14\n","21619/21619 [==============================] - 26s 1ms/step - loss: 0.0081 - val_loss: 0.0060\n","Epoch 6/14\n","21619/21619 [==============================] - 26s 1ms/step - loss: 0.0080 - val_loss: 0.0058\n","Epoch 7/14\n","21619/21619 [==============================] - 26s 1ms/step - loss: 0.0080 - val_loss: 0.0060\n","Epoch 8/14\n","21568/21619 [============================>.] - ETA: 0s - loss: 0.0079\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21619/21619 [==============================] - 33s 2ms/step - loss: 0.0079 - val_loss: 0.0060\n","Epoch 9/14\n","21619/21619 [==============================] - 29s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 10/14\n","21568/21619 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21619/21619 [==============================] - 28s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 11/14\n","21619/21619 [==============================] - 27s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 12/14\n","21600/21619 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21619/21619 [==============================] - 28s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 13/14\n","21619/21619 [==============================] - 27s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 14/14\n","21600/21619 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21619/21619 [==============================] - 28s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_104 (InputLayer)       (None, 46, 7)             0         \n","_________________________________________________________________\n","lstm_104 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_104 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21616 samples, validate on 5369 samples\n","Epoch 1/14\n","21616/21616 [==============================] - 43s 2ms/step - loss: 0.0117 - val_loss: 0.0073\n","Epoch 2/14\n","21616/21616 [==============================] - 30s 1ms/step - loss: 0.0093 - val_loss: 0.0068\n","Epoch 3/14\n","21616/21616 [==============================] - 29s 1ms/step - loss: 0.0088 - val_loss: 0.0063\n","Epoch 4/14\n","21616/21616 [==============================] - 30s 1ms/step - loss: 0.0085 - val_loss: 0.0061\n","Epoch 5/14\n","21616/21616 [==============================] - 29s 1ms/step - loss: 0.0082 - val_loss: 0.0062\n","Epoch 6/14\n","21616/21616 [==============================] - 29s 1ms/step - loss: 0.0081 - val_loss: 0.0064\n","Epoch 7/14\n","21616/21616 [==============================] - 29s 1ms/step - loss: 0.0080 - val_loss: 0.0059\n","Epoch 8/14\n","21600/21616 [============================>.] - ETA: 0s - loss: 0.0079\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21616/21616 [==============================] - 33s 2ms/step - loss: 0.0079 - val_loss: 0.0059\n","Epoch 9/14\n","21616/21616 [==============================] - 30s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 10/14\n","21600/21616 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21616/21616 [==============================] - 30s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 11/14\n","21616/21616 [==============================] - 29s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 12/14\n","21568/21616 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21616/21616 [==============================] - 29s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 13/14\n","21616/21616 [==============================] - 29s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 14/14\n","21600/21616 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21616/21616 [==============================] - 29s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_105 (InputLayer)       (None, 49, 7)             0         \n","_________________________________________________________________\n","lstm_105 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_105 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21613 samples, validate on 5366 samples\n","Epoch 1/14\n","21613/21613 [==============================] - 43s 2ms/step - loss: 0.0112 - val_loss: 0.0072\n","Epoch 2/14\n","21613/21613 [==============================] - 30s 1ms/step - loss: 0.0092 - val_loss: 0.0069\n","Epoch 3/14\n","21613/21613 [==============================] - 29s 1ms/step - loss: 0.0087 - val_loss: 0.0063\n","Epoch 4/14\n","21613/21613 [==============================] - 29s 1ms/step - loss: 0.0083 - val_loss: 0.0060\n","Epoch 5/14\n","21613/21613 [==============================] - 29s 1ms/step - loss: 0.0081 - val_loss: 0.0061\n","Epoch 6/14\n","21613/21613 [==============================] - 30s 1ms/step - loss: 0.0080 - val_loss: 0.0059\n","Epoch 7/14\n","21613/21613 [==============================] - 30s 1ms/step - loss: 0.0079 - val_loss: 0.0058\n","Epoch 8/14\n","21568/21613 [============================>.] - ETA: 0s - loss: 0.0078\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21613/21613 [==============================] - 33s 2ms/step - loss: 0.0078 - val_loss: 0.0059\n","Epoch 9/14\n","21613/21613 [==============================] - 29s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 10/14\n","21568/21613 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21613/21613 [==============================] - 29s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 11/14\n","21613/21613 [==============================] - 29s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 12/14\n","21568/21613 [============================>.] - ETA: 0s - loss: 0.0074\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21613/21613 [==============================] - 29s 1ms/step - loss: 0.0074 - val_loss: 0.0056\n","Epoch 13/14\n","21613/21613 [==============================] - 29s 1ms/step - loss: 0.0074 - val_loss: 0.0056\n","Epoch 14/14\n","21568/21613 [============================>.] - ETA: 0s - loss: 0.0074\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21613/21613 [==============================] - 29s 1ms/step - loss: 0.0074 - val_loss: 0.0056\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_106 (InputLayer)       (None, 52, 7)             0         \n","_________________________________________________________________\n","lstm_106 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_106 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["Train on 21610 samples, validate on 5363 samples\n","Epoch 1/14\n","21610/21610 [==============================] - 45s 2ms/step - loss: 0.0125 - val_loss: 0.0081\n","Epoch 2/14\n","21610/21610 [==============================] - 32s 1ms/step - loss: 0.0094 - val_loss: 0.0077\n","Epoch 3/14\n","21610/21610 [==============================] - 31s 1ms/step - loss: 0.0088 - val_loss: 0.0063\n","Epoch 4/14\n","21610/21610 [==============================] - 31s 1ms/step - loss: 0.0083 - val_loss: 0.0059\n","Epoch 5/14\n","21610/21610 [==============================] - 31s 1ms/step - loss: 0.0081 - val_loss: 0.0062\n","Epoch 6/14\n","21610/21610 [==============================] - 31s 1ms/step - loss: 0.0080 - val_loss: 0.0059\n","Epoch 7/14\n","21568/21610 [============================>.] - ETA: 0s - loss: 0.0079\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21610/21610 [==============================] - 35s 2ms/step - loss: 0.0079 - val_loss: 0.0059\n","Epoch 8/14\n","21610/21610 [==============================] - 31s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 9/14\n","21568/21610 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21610/21610 [==============================] - 31s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 10/14\n","21610/21610 [==============================] - 30s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 11/14\n","21600/21610 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21610/21610 [==============================] - 30s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 12/14\n","21610/21610 [==============================] - 31s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 13/14\n","21568/21610 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21610/21610 [==============================] - 30s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 14/14\n","21610/21610 [==============================] - 30s 1ms/step - loss: 0.0075 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_107 (InputLayer)       (None, 55, 7)             0         \n","_________________________________________________________________\n","lstm_107 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_107 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21607 samples, validate on 5360 samples\n","Epoch 1/14\n","21607/21607 [==============================] - 45s 2ms/step - loss: 0.0129 - val_loss: 0.0093\n","Epoch 2/14\n","21607/21607 [==============================] - 31s 1ms/step - loss: 0.0094 - val_loss: 0.0065\n","Epoch 3/14\n","21607/21607 [==============================] - 31s 1ms/step - loss: 0.0086 - val_loss: 0.0061\n","Epoch 4/14\n","21607/21607 [==============================] - 31s 1ms/step - loss: 0.0082 - val_loss: 0.0065\n","Epoch 5/14\n","21607/21607 [==============================] - 31s 1ms/step - loss: 0.0082 - val_loss: 0.0065\n","Epoch 6/14\n","21607/21607 [==============================] - 31s 1ms/step - loss: 0.0080 - val_loss: 0.0062\n","Epoch 7/14\n","21600/21607 [============================>.] - ETA: 0s - loss: 0.0079\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21607/21607 [==============================] - 35s 2ms/step - loss: 0.0079 - val_loss: 0.0061\n","Epoch 8/14\n","21607/21607 [==============================] - 32s 1ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 9/14\n","21600/21607 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21607/21607 [==============================] - 1387s 64ms/step - loss: 0.0076 - val_loss: 0.0058\n","Epoch 10/14\n","21607/21607 [==============================] - 40s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 11/14\n","21568/21607 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21607/21607 [==============================] - 33s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 12/14\n","21607/21607 [==============================] - 33s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 13/14\n","21568/21607 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21607/21607 [==============================] - 33s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 14/14\n","21607/21607 [==============================] - 33s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_108 (InputLayer)       (None, 58, 7)             0         \n","_________________________________________________________________\n","lstm_108 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_108 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21604 samples, validate on 5357 samples\n","Epoch 1/14\n","21604/21604 [==============================] - 52s 2ms/step - loss: 0.0122 - val_loss: 0.0081\n","Epoch 2/14\n","21604/21604 [==============================] - 35s 2ms/step - loss: 0.0094 - val_loss: 0.0069\n","Epoch 3/14\n","21604/21604 [==============================] - 40s 2ms/step - loss: 0.0089 - val_loss: 0.0067\n","Epoch 4/14\n","21604/21604 [==============================] - 36s 2ms/step - loss: 0.0086 - val_loss: 0.0063\n","Epoch 5/14\n","21604/21604 [==============================] - 38s 2ms/step - loss: 0.0083 - val_loss: 0.0062\n","Epoch 6/14\n","21604/21604 [==============================] - 33s 2ms/step - loss: 0.0081 - val_loss: 0.0059\n","Epoch 7/14\n","21604/21604 [==============================] - 36s 2ms/step - loss: 0.0080 - val_loss: 0.0060\n","Epoch 8/14\n","21600/21604 [============================>.] - ETA: 0s - loss: 0.0079\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21604/21604 [==============================] - 40s 2ms/step - loss: 0.0079 - val_loss: 0.0059\n","Epoch 9/14\n","21604/21604 [==============================] - 34s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 10/14\n","21568/21604 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21604/21604 [==============================] - 32s 1ms/step - loss: 0.0076 - val_loss: 0.0058\n","Epoch 11/14\n","21604/21604 [==============================] - 40s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 12/14\n","21600/21604 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21604/21604 [==============================] - 35s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 13/14\n","21604/21604 [==============================] - 38s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 14/14\n","21600/21604 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21604/21604 [==============================] - 38s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_109 (InputLayer)       (None, 61, 7)             0         \n","_________________________________________________________________\n","lstm_109 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_109 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["Train on 21601 samples, validate on 5354 samples\n","Epoch 1/14\n","21601/21601 [==============================] - 61s 3ms/step - loss: 0.0130 - val_loss: 0.0085\n","Epoch 2/14\n","21601/21601 [==============================] - 38s 2ms/step - loss: 0.0096 - val_loss: 0.0068\n","Epoch 3/14\n","21601/21601 [==============================] - 40s 2ms/step - loss: 0.0089 - val_loss: 0.0064\n","Epoch 4/14\n","21601/21601 [==============================] - 37s 2ms/step - loss: 0.0086 - val_loss: 0.0061\n","Epoch 5/14\n","21601/21601 [==============================] - 38s 2ms/step - loss: 0.0083 - val_loss: 0.0062\n","Epoch 6/14\n","21601/21601 [==============================] - 39s 2ms/step - loss: 0.0081 - val_loss: 0.0060\n","Epoch 7/14\n","21601/21601 [==============================] - 38s 2ms/step - loss: 0.0081 - val_loss: 0.0060\n","Epoch 8/14\n","21568/21601 [============================>.] - ETA: 0s - loss: 0.0080\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21601/21601 [==============================] - 40s 2ms/step - loss: 0.0080 - val_loss: 0.0058\n","Epoch 9/14\n","21601/21601 [==============================] - 33s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 10/14\n","21600/21601 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21601/21601 [==============================] - 33s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 11/14\n","21601/21601 [==============================] - 43s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 12/14\n","21600/21601 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21601/21601 [==============================] - 46s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 13/14\n","21601/21601 [==============================] - 38s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 14/14\n","21600/21601 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21601/21601 [==============================] - 40s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_110 (InputLayer)       (None, 64, 7)             0         \n","_________________________________________________________________\n","lstm_110 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_110 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21598 samples, validate on 5351 samples\n","Epoch 1/14\n","21598/21598 [==============================] - 60s 3ms/step - loss: 0.0122 - val_loss: 0.0072\n","Epoch 2/14\n","21598/21598 [==============================] - 40s 2ms/step - loss: 0.0092 - val_loss: 0.0067\n","Epoch 3/14\n","21598/21598 [==============================] - 41s 2ms/step - loss: 0.0087 - val_loss: 0.0067\n","Epoch 4/14\n","21598/21598 [==============================] - 35s 2ms/step - loss: 0.0084 - val_loss: 0.0063\n","Epoch 5/14\n","21536/21598 [============================>.] - ETA: 0s - loss: 0.0082\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21598/21598 [==============================] - 40s 2ms/step - loss: 0.0082 - val_loss: 0.0061\n","Epoch 6/14\n","21598/21598 [==============================] - 38s 2ms/step - loss: 0.0078 - val_loss: 0.0059\n","Epoch 7/14\n","21598/21598 [==============================] - 38s 2ms/step - loss: 0.0078 - val_loss: 0.0059\n","Epoch 8/14\n","21598/21598 [==============================] - 43s 2ms/step - loss: 0.0078 - val_loss: 0.0059\n","Epoch 9/14\n","21536/21598 [============================>.] - ETA: 0s - loss: 0.0077\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21598/21598 [==============================] - 34s 2ms/step - loss: 0.0078 - val_loss: 0.0059\n","Epoch 10/14\n","21598/21598 [==============================] - 38s 2ms/step - loss: 0.0077 - val_loss: 0.0059\n","Epoch 11/14\n","21568/21598 [============================>.] - ETA: 0s - loss: 0.0077\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21598/21598 [==============================] - 40s 2ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 12/14\n","21598/21598 [==============================] - 40s 2ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 13/14\n","21568/21598 [============================>.] - ETA: 0s - loss: 0.0077\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21598/21598 [==============================] - 41s 2ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 14/14\n","21598/21598 [==============================] - 37s 2ms/step - loss: 0.0077 - val_loss: 0.0058\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_111 (InputLayer)       (None, 67, 7)             0         \n","_________________________________________________________________\n","lstm_111 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_111 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21595 samples, validate on 5348 samples\n","Epoch 1/14\n","21595/21595 [==============================] - 52s 2ms/step - loss: 0.0115 - val_loss: 0.0071\n","Epoch 2/14\n","21595/21595 [==============================] - 37s 2ms/step - loss: 0.0091 - val_loss: 0.0077\n","Epoch 3/14\n","21595/21595 [==============================] - 38s 2ms/step - loss: 0.0087 - val_loss: 0.0062\n","Epoch 4/14\n","21595/21595 [==============================] - 37s 2ms/step - loss: 0.0083 - val_loss: 0.0061\n","Epoch 5/14\n","21536/21595 [============================>.] - ETA: 0s - loss: 0.0081\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21595/21595 [==============================] - 43s 2ms/step - loss: 0.0081 - val_loss: 0.0064\n","Epoch 6/14\n","21595/21595 [==============================] - 38s 2ms/step - loss: 0.0078 - val_loss: 0.0058\n","Epoch 7/14\n","21595/21595 [==============================] - 40s 2ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 8/14\n","21595/21595 [==============================] - 39s 2ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 9/14\n","21568/21595 [============================>.] - ETA: 0s - loss: 0.0077\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21595/21595 [==============================] - 40s 2ms/step - loss: 0.0077 - val_loss: 0.0057\n","Epoch 10/14\n","21595/21595 [==============================] - 39s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 11/14\n","21568/21595 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21595/21595 [==============================] - 41s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 12/14\n","21595/21595 [==============================] - 46s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 13/14\n","21568/21595 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21595/21595 [==============================] - 40s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 14/14\n","21595/21595 [==============================] - 44s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_112 (InputLayer)       (None, 70, 7)             0         \n","_________________________________________________________________\n","lstm_112 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_112 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["Train on 21592 samples, validate on 5345 samples\n","Epoch 1/14\n","21592/21592 [==============================] - 59s 3ms/step - loss: 0.0117 - val_loss: 0.0077\n","Epoch 2/14\n","21592/21592 [==============================] - 43s 2ms/step - loss: 0.0093 - val_loss: 0.0066\n","Epoch 3/14\n","21592/21592 [==============================] - 45s 2ms/step - loss: 0.0087 - val_loss: 0.0066\n","Epoch 4/14\n","21592/21592 [==============================] - 43s 2ms/step - loss: 0.0084 - val_loss: 0.0062\n","Epoch 5/14\n","21592/21592 [==============================] - 39s 2ms/step - loss: 0.0081 - val_loss: 0.0060\n","Epoch 6/14\n","21592/21592 [==============================] - 42s 2ms/step - loss: 0.0080 - val_loss: 0.0060\n","Epoch 7/14\n","21592/21592 [==============================] - 42s 2ms/step - loss: 0.0079 - val_loss: 0.0059\n","Epoch 8/14\n","21568/21592 [============================>.] - ETA: 0s - loss: 0.0079\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21592/21592 [==============================] - 47s 2ms/step - loss: 0.0079 - val_loss: 0.0060\n","Epoch 9/14\n","21592/21592 [==============================] - 46s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 10/14\n","21568/21592 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21592/21592 [==============================] - 43s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 11/14\n","21592/21592 [==============================] - 42s 2ms/step - loss: 0.0075 - val_loss: 0.0056\n","Epoch 12/14\n","21568/21592 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21592/21592 [==============================] - 42s 2ms/step - loss: 0.0075 - val_loss: 0.0056\n","Epoch 13/14\n","21592/21592 [==============================] - 44s 2ms/step - loss: 0.0075 - val_loss: 0.0056\n","Epoch 14/14\n","21568/21592 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21592/21592 [==============================] - 42s 2ms/step - loss: 0.0075 - val_loss: 0.0056\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_113 (InputLayer)       (None, 73, 7)             0         \n","_________________________________________________________________\n","lstm_113 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_113 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21589 samples, validate on 5342 samples\n","Epoch 1/14\n","21589/21589 [==============================] - 61s 3ms/step - loss: 0.0115 - val_loss: 0.0071\n","Epoch 2/14\n","21589/21589 [==============================] - 46s 2ms/step - loss: 0.0091 - val_loss: 0.0072\n","Epoch 3/14\n","21589/21589 [==============================] - 43s 2ms/step - loss: 0.0086 - val_loss: 0.0065\n","Epoch 4/14\n","21589/21589 [==============================] - 44s 2ms/step - loss: 0.0084 - val_loss: 0.0061\n","Epoch 5/14\n","21568/21589 [============================>.] - ETA: 0s - loss: 0.0082\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21589/21589 [==============================] - 50s 2ms/step - loss: 0.0082 - val_loss: 0.0061\n","Epoch 6/14\n","21589/21589 [==============================] - 44s 2ms/step - loss: 0.0078 - val_loss: 0.0058\n","Epoch 7/14\n","21589/21589 [==============================] - 44s 2ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 8/14\n","21589/21589 [==============================] - 43s 2ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 9/14\n","21568/21589 [============================>.] - ETA: 0s - loss: 0.0077\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21589/21589 [==============================] - 44s 2ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 10/14\n","21589/21589 [==============================] - 43s 2ms/step - loss: 0.0077 - val_loss: 0.0057\n","Epoch 11/14\n","21568/21589 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21589/21589 [==============================] - 44s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 12/14\n","21589/21589 [==============================] - 44s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 13/14\n","21568/21589 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21589/21589 [==============================] - 44s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 14/14\n","21589/21589 [==============================] - 45s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_114 (InputLayer)       (None, 76, 7)             0         \n","_________________________________________________________________\n","lstm_114 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_114 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21586 samples, validate on 5339 samples\n","Epoch 1/14\n","21586/21586 [==============================] - 65s 3ms/step - loss: 0.0110 - val_loss: 0.0073\n","Epoch 2/14\n","21586/21586 [==============================] - 46s 2ms/step - loss: 0.0091 - val_loss: 0.0074\n","Epoch 3/14\n","21586/21586 [==============================] - 47s 2ms/step - loss: 0.0086 - val_loss: 0.0063\n","Epoch 4/14\n","21586/21586 [==============================] - 45s 2ms/step - loss: 0.0083 - val_loss: 0.0069\n","Epoch 5/14\n","21586/21586 [==============================] - 45s 2ms/step - loss: 0.0081 - val_loss: 0.0058\n","Epoch 6/14\n","21586/21586 [==============================] - 46s 2ms/step - loss: 0.0080 - val_loss: 0.0058\n","Epoch 7/14\n","21586/21586 [==============================] - 46s 2ms/step - loss: 0.0078 - val_loss: 0.0060\n","Epoch 8/14\n","21568/21586 [============================>.] - ETA: 0s - loss: 0.0078\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21586/21586 [==============================] - 51s 2ms/step - loss: 0.0078 - val_loss: 0.0059\n","Epoch 9/14\n","21586/21586 [==============================] - 47s 2ms/step - loss: 0.0075 - val_loss: 0.0056\n","Epoch 10/14\n","21568/21586 [============================>.] - ETA: 0s - loss: 0.0074\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21586/21586 [==============================] - 47s 2ms/step - loss: 0.0074 - val_loss: 0.0056\n","Epoch 11/14\n","21586/21586 [==============================] - 49s 2ms/step - loss: 0.0074 - val_loss: 0.0056\n","Epoch 12/14\n","21568/21586 [============================>.] - ETA: 0s - loss: 0.0074\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21586/21586 [==============================] - 46s 2ms/step - loss: 0.0074 - val_loss: 0.0056\n","Epoch 13/14\n","21586/21586 [==============================] - 44s 2ms/step - loss: 0.0074 - val_loss: 0.0056\n","Epoch 14/14\n","21568/21586 [============================>.] - ETA: 0s - loss: 0.0074\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21586/21586 [==============================] - 47s 2ms/step - loss: 0.0074 - val_loss: 0.0056\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_115 (InputLayer)       (None, 79, 7)             0         \n","_________________________________________________________________\n","lstm_115 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_115 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["Train on 21583 samples, validate on 5336 samples\n","Epoch 1/14\n","21583/21583 [==============================] - 71s 3ms/step - loss: 0.0118 - val_loss: 0.0072\n","Epoch 2/14\n","21583/21583 [==============================] - 52s 2ms/step - loss: 0.0091 - val_loss: 0.0068\n","Epoch 3/14\n","21583/21583 [==============================] - 55s 3ms/step - loss: 0.0085 - val_loss: 0.0063\n","Epoch 4/14\n","21583/21583 [==============================] - 62s 3ms/step - loss: 0.0082 - val_loss: 0.0062\n","Epoch 5/14\n","21583/21583 [==============================] - 61s 3ms/step - loss: 0.0081 - val_loss: 0.0059\n","Epoch 6/14\n","21583/21583 [==============================] - 57s 3ms/step - loss: 0.0079 - val_loss: 0.0061\n","Epoch 7/14\n","21583/21583 [==============================] - 53s 2ms/step - loss: 0.0079 - val_loss: 0.0059\n","Epoch 8/14\n","21568/21583 [============================>.] - ETA: 0s - loss: 0.0079\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21583/21583 [==============================] - 56s 3ms/step - loss: 0.0079 - val_loss: 0.0058\n","Epoch 9/14\n","21583/21583 [==============================] - 52s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 10/14\n","21568/21583 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21583/21583 [==============================] - 51s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 11/14\n","21583/21583 [==============================] - 52s 2ms/step - loss: 0.0075 - val_loss: 0.0056\n","Epoch 12/14\n","21568/21583 [============================>.] - ETA: 0s - loss: 0.0074\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21583/21583 [==============================] - 51s 2ms/step - loss: 0.0074 - val_loss: 0.0057\n","Epoch 13/14\n","21583/21583 [==============================] - 51s 2ms/step - loss: 0.0074 - val_loss: 0.0057\n","Epoch 14/14\n","21568/21583 [============================>.] - ETA: 0s - loss: 0.0074\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21583/21583 [==============================] - 50s 2ms/step - loss: 0.0074 - val_loss: 0.0056\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_116 (InputLayer)       (None, 82, 7)             0         \n","_________________________________________________________________\n","lstm_116 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_116 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21580 samples, validate on 5333 samples\n","Epoch 1/14\n","21580/21580 [==============================] - 68s 3ms/step - loss: 0.0136 - val_loss: 0.0075\n","Epoch 2/14\n","21580/21580 [==============================] - 52s 2ms/step - loss: 0.0096 - val_loss: 0.0068\n","Epoch 3/14\n","21580/21580 [==============================] - 52s 2ms/step - loss: 0.0088 - val_loss: 0.0064\n","Epoch 4/14\n","21580/21580 [==============================] - 51s 2ms/step - loss: 0.0084 - val_loss: 0.0061\n","Epoch 5/14\n","21580/21580 [==============================] - 52s 2ms/step - loss: 0.0081 - val_loss: 0.0061\n","Epoch 6/14\n","21580/21580 [==============================] - 57s 3ms/step - loss: 0.0080 - val_loss: 0.0059\n","Epoch 7/14\n","21568/21580 [============================>.] - ETA: 0s - loss: 0.0080\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21580/21580 [==============================] - 61s 3ms/step - loss: 0.0080 - val_loss: 0.0059\n","Epoch 8/14\n","21580/21580 [==============================] - 50s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 9/14\n","21568/21580 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21580/21580 [==============================] - 52s 2ms/step - loss: 0.0076 - val_loss: 0.0058\n","Epoch 10/14\n","21580/21580 [==============================] - 54s 3ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 11/14\n","21568/21580 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21580/21580 [==============================] - 49s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 12/14\n","21580/21580 [==============================] - 54s 3ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 13/14\n","21568/21580 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21580/21580 [==============================] - 56s 3ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 14/14\n","21580/21580 [==============================] - 52s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_117 (InputLayer)       (None, 85, 7)             0         \n","_________________________________________________________________\n","lstm_117 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_117 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21577 samples, validate on 5330 samples\n","Epoch 1/14\n","21577/21577 [==============================] - 78s 4ms/step - loss: 0.0123 - val_loss: 0.0074\n","Epoch 2/14\n","21577/21577 [==============================] - 60s 3ms/step - loss: 0.0093 - val_loss: 0.0071\n","Epoch 3/14\n","21577/21577 [==============================] - 53s 2ms/step - loss: 0.0088 - val_loss: 0.0064\n","Epoch 4/14\n","21577/21577 [==============================] - 57s 3ms/step - loss: 0.0086 - val_loss: 0.0062\n","Epoch 5/14\n","21577/21577 [==============================] - 59s 3ms/step - loss: 0.0083 - val_loss: 0.0068\n","Epoch 6/14\n","21577/21577 [==============================] - 57s 3ms/step - loss: 0.0081 - val_loss: 0.0069\n","Epoch 7/14\n","21577/21577 [==============================] - 61s 3ms/step - loss: 0.0080 - val_loss: 0.0059\n","Epoch 8/14\n","21568/21577 [============================>.] - ETA: 0s - loss: 0.0079\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21577/21577 [==============================] - 59s 3ms/step - loss: 0.0079 - val_loss: 0.0060\n","Epoch 9/14\n","21577/21577 [==============================] - 53s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 10/14\n","21568/21577 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21577/21577 [==============================] - 53s 2ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 11/14\n","21577/21577 [==============================] - 55s 3ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 12/14\n","21568/21577 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21577/21577 [==============================] - 61s 3ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 13/14\n","21577/21577 [==============================] - 59s 3ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 14/14\n","21568/21577 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21577/21577 [==============================] - 59s 3ms/step - loss: 0.0075 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_118 (InputLayer)       (None, 88, 7)             0         \n","_________________________________________________________________\n","lstm_118 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_118 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["Train on 21574 samples, validate on 5327 samples\n","Epoch 1/14\n","21574/21574 [==============================] - 68s 3ms/step - loss: 0.0114 - val_loss: 0.0073\n","Epoch 2/14\n","21574/21574 [==============================] - 48s 2ms/step - loss: 0.0090 - val_loss: 0.0065\n","Epoch 3/14\n","21574/21574 [==============================] - 48s 2ms/step - loss: 0.0086 - val_loss: 0.0064\n","Epoch 4/14\n","21574/21574 [==============================] - 54s 3ms/step - loss: 0.0083 - val_loss: 0.0062\n","Epoch 5/14\n","21568/21574 [============================>.] - ETA: 0s - loss: 0.0081\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21574/21574 [==============================] - 55s 3ms/step - loss: 0.0081 - val_loss: 0.0060\n","Epoch 6/14\n","21574/21574 [==============================] - 47s 2ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 7/14\n","21574/21574 [==============================] - 48s 2ms/step - loss: 0.0077 - val_loss: 0.0057\n","Epoch 8/14\n","21574/21574 [==============================] - 50s 2ms/step - loss: 0.0077 - val_loss: 0.0057\n","Epoch 9/14\n","21568/21574 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21574/21574 [==============================] - 54s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 10/14\n","21574/21574 [==============================] - 55s 3ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 11/14\n","21568/21574 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21574/21574 [==============================] - 53s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 12/14\n","21574/21574 [==============================] - 56s 3ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 13/14\n","21568/21574 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21574/21574 [==============================] - 55s 3ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 14/14\n","21574/21574 [==============================] - 54s 3ms/step - loss: 0.0076 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_119 (InputLayer)       (None, 91, 7)             0         \n","_________________________________________________________________\n","lstm_119 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_119 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21571 samples, validate on 5324 samples\n","Epoch 1/14\n","21571/21571 [==============================] - 73s 3ms/step - loss: 0.0115 - val_loss: 0.0070\n","Epoch 2/14\n","21571/21571 [==============================] - 57s 3ms/step - loss: 0.0090 - val_loss: 0.0065\n","Epoch 3/14\n","21571/21571 [==============================] - 56s 3ms/step - loss: 0.0085 - val_loss: 0.0061\n","Epoch 4/14\n","21571/21571 [==============================] - 58s 3ms/step - loss: 0.0082 - val_loss: 0.0061\n","Epoch 5/14\n","21568/21571 [============================>.] - ETA: 0s - loss: 0.0081\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21571/21571 [==============================] - 61s 3ms/step - loss: 0.0081 - val_loss: 0.0062\n","Epoch 6/14\n","21571/21571 [==============================] - 58s 3ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 7/14\n","21571/21571 [==============================] - 54s 3ms/step - loss: 0.0077 - val_loss: 0.0058\n","Epoch 8/14\n","21571/21571 [==============================] - 54s 3ms/step - loss: 0.0076 - val_loss: 0.0058\n","Epoch 9/14\n","21568/21571 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21571/21571 [==============================] - 54s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 10/14\n","21571/21571 [==============================] - 65s 3ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 11/14\n","21568/21571 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21571/21571 [==============================] - 59s 3ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 12/14\n","21571/21571 [==============================] - 52s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 13/14\n","21568/21571 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21571/21571 [==============================] - 51s 2ms/step - loss: 0.0076 - val_loss: 0.0057\n","Epoch 14/14\n","21571/21571 [==============================] - 56s 3ms/step - loss: 0.0076 - val_loss: 0.0057\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_120 (InputLayer)       (None, 94, 7)             0         \n","_________________________________________________________________\n","lstm_120 (LSTM)              (None, 50)                11600     \n","_________________________________________________________________\n","dense_120 (Dense)            (None, 1)                 51        \n","=================================================================\n","Total params: 11,651\n","Trainable params: 11,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21568 samples, validate on 5321 samples\n","Epoch 1/14\n","21568/21568 [==============================] - 73s 3ms/step - loss: 0.0122 - val_loss: 0.0077\n","Epoch 2/14\n","21568/21568 [==============================] - 56s 3ms/step - loss: 0.0092 - val_loss: 0.0070\n","Epoch 3/14\n","21568/21568 [==============================] - 56s 3ms/step - loss: 0.0087 - val_loss: 0.0064\n","Epoch 4/14\n","21568/21568 [==============================] - 56s 3ms/step - loss: 0.0083 - val_loss: 0.0059\n","Epoch 5/14\n","21568/21568 [==============================] - 56s 3ms/step - loss: 0.0081 - val_loss: 0.0059\n","Epoch 6/14\n","21568/21568 [==============================] - 56s 3ms/step - loss: 0.0080 - val_loss: 0.0059\n","Epoch 7/14\n","21568/21568 [==============================] - 57s 3ms/step - loss: 0.0079 - val_loss: 0.0058\n","Epoch 8/14\n","21536/21568 [============================>.] - ETA: 0s - loss: 0.0079\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21568/21568 [==============================] - 62s 3ms/step - loss: 0.0079 - val_loss: 0.0059\n","Epoch 9/14\n","21568/21568 [==============================] - 56s 3ms/step - loss: 0.0076 - val_loss: 0.0056\n","Epoch 10/14\n","21536/21568 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21568/21568 [==============================] - 56s 3ms/step - loss: 0.0075 - val_loss: 0.0057\n","Epoch 11/14\n","21568/21568 [==============================] - 57s 3ms/step - loss: 0.0075 - val_loss: 0.0056\n","Epoch 12/14\n","21536/21568 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21568/21568 [==============================] - 56s 3ms/step - loss: 0.0075 - val_loss: 0.0056\n","Epoch 13/14\n","21568/21568 [==============================] - 56s 3ms/step - loss: 0.0075 - val_loss: 0.0056\n","Epoch 14/14\n","21536/21568 [============================>.] - ETA: 0s - loss: 0.0075\n","Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","21568/21568 [==============================] - 56s 3ms/step - loss: 0.0075 - val_loss: 0.0056\n","\n","*** BEST RESULT WITH 76 HISTORY HOURS WITH RMSE OF: 0.0746921123307941 ***\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZIAAAEXCAYAAACH/8KRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nANJj6iEAAAAASUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x25a46088b70>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"zzIgEq0TcxnZ","colab_type":"text"},"cell_type":"markdown","source":["in this plot we can notice that for different n_in we got different rmse, the lowest rmse we got for 76 n_in.  \n","let's check the rmse:"]},{"metadata":{"scrolled":true,"id":"OZ3x68HUcxnZ","colab_type":"code","outputId":"ac7408a5-055e-476e-f729-7453c3e7deb6","colab":{}},"cell_type":"code","source":["best_m.load_weights('./models_weights/_best_n_in_76/best_weights.hdf5')\n","validation_X,validation_Y = to_supervised(validation_normalized,76)\n","nn_pred = best_m.predict(np.asarray(validation_X))\n","rmse(validation_Y,nn_pred)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0746921123307941"]},"metadata":{"tags":[]},"execution_count":380}]},{"metadata":{"id":"YvvtQHAQcxnb","colab_type":"text"},"cell_type":"markdown","source":["the last result was depend on n_in let's get result for our test set:"]},{"metadata":{"id":"PnCuh69-cxnb","colab_type":"code","outputId":"ffc996ed-48e6-4c9f-d103-78fc48e46651","colab":{}},"cell_type":"code","source":["best_m.load_weights('./models_weights/_best_n_in_76/best_weights.hdf5')\n","X,Y = to_supervised(test_data_normalized,76)\n","rmse(Y,best_m.predict(np.asarray(X)))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.07512431310332386"]},"metadata":{"tags":[]},"execution_count":381}]},{"metadata":{"id":"ScjNzfsIcxnd","colab_type":"text"},"cell_type":"markdown","source":["as we can see we have got higher rmse than in the validation set. but the gap is very small.  \n","we improved the rmse from 0.0763 to 0.0751 ,*** so our new benchmark is 0.0751 ***"]},{"metadata":{"id":"FeCY9Jhucxnd","colab_type":"code","outputId":"b4d408c6-aee8-4cbd-a91d-851340008ffa","colab":{}},"cell_type":"code","source":["n_in=76; path = './models_weights/_best_n_in_76/best_weights.hdf5'\n","analyze_result(model,path,n_in)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hour</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>19</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>18</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>12</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>14</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>21</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>20</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>11</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>10</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>22</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>17</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>15</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>13</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>23</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    hour  count\n","0     19      8\n","1     18      8\n","2     12      7\n","3     14      4\n","4     21      3\n","5     20      3\n","6     11      3\n","7     10      3\n","8     22      2\n","9     17      2\n","10    15      2\n","11    13      2\n","12    23      1\n","13     8      1"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>pm</th>\n","      <td>35</td>\n","    </tr>\n","    <tr>\n","      <th>am</th>\n","      <td>14</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     0\n","pm  35\n","am  14"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>day</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Saturday</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sunday</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Wednesday</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Monday</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Thursday</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Tuesday</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Friday</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         day  count\n","0   Saturday     12\n","1     Sunday     11\n","2  Wednesday      8\n","3     Monday      8\n","4   Thursday      5\n","5    Tuesday      4\n","6     Friday      1"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"EtQTRlAXcxng","colab_type":"text"},"cell_type":"markdown","source":["* there are only 49 errors.  \n","* 70% at pm.\n","* 26% during weekend.  \n","in compare to the last model there are only 49 errors with 0.2 above the rmse (54 in last model),\n","most of the errors are still at pm, but the number of errors during the weekend dropped from 40% to 26%. so by this model we improved the weekend prediction."]},{"metadata":{"collapsed":true,"id":"J5Wk2nJ4cxnh","colab_type":"text"},"cell_type":"markdown","source":["## deeper nn using 1D cnn  \n","now we try to use Conv1D in order to recognize patterns in the data, we also use LSTM to study and 'remember' these sequence of patterns. we use dropout because the number of parameters is very high and we dont want to overfit the data."]},{"metadata":{"scrolled":true,"id":"UpO6sxu_cxnh","colab_type":"code","outputId":"5cbd2bf2-22b0-44b6-d03d-285fa2993996","colab":{}},"cell_type":"code","source":["def build_nn_model_v2(n_in,n_out,n_vars):\n","    inp = Input(shape=(n_in,n_vars))\n","    x = Conv1D(64,3)(inp)\n","    x = Dropout(0.1)(x)\n","    x = LSTM(40, return_sequences = False)(x)\n","    output = Dense(n_out)(x)\n","    model = Model(inp,output)\n","    model.compile(loss='mean_squared_error',optimizer='adam')\n","    model.summary()\n","    return model\n","\n","_path = './models_weights/'\n","_description = '_CNN_new16'\n","n_out = 1\n","n_in = 76 \n","n_vars=validation_normalized.shape[1]\n","\n","# series to supervised\n","validation_X,validation_Y = to_supervised(validation_normalized,n_in)\n","train_X,train_Y = to_supervised(train_normalized,n_in)\n","\n","# build NN model\n","model = build_nn_model_v2(n_in,n_out,n_vars)\n","\n","# fit the model\n","history = model.fit(np.asarray(train_X),np.asarray(train_Y),validation_data=[np.asarray(validation_X),np.asarray(validation_Y)],\n","          callbacks = set_callbacks(patience=5, path=_path , description=_description + '_' + str(n_in) + '_new'),epochs=13,batch_size=32) \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_23 (InputLayer)        (None, 76, 7)             0         \n","_________________________________________________________________\n","conv1d_7 (Conv1D)            (None, 74, 64)            1408      \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 74, 64)            0         \n","_________________________________________________________________\n","lstm_28 (LSTM)               (None, 40)                16800     \n","_________________________________________________________________\n","dense_38 (Dense)             (None, 1)                 41        \n","=================================================================\n","Total params: 18,249\n","Trainable params: 18,249\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 21586 samples, validate on 5339 samples\n","Epoch 1/13\n","21586/21586 [==============================] - 54s 3ms/step - loss: 0.0117 - val_loss: 0.0071\n","Epoch 2/13\n","21586/21586 [==============================] - 50s 2ms/step - loss: 0.0094 - val_loss: 0.0067\n","Epoch 3/13\n","21586/21586 [==============================] - 53s 2ms/step - loss: 0.0089 - val_loss: 0.0062\n","Epoch 4/13\n","21586/21586 [==============================] - 57s 3ms/step - loss: 0.0085 - val_loss: 0.0061\n","Epoch 5/13\n","21586/21586 [==============================] - 58s 3ms/step - loss: 0.0083 - val_loss: 0.0061\n","Epoch 6/13\n","21586/21586 [==============================] - 57s 3ms/step - loss: 0.0081 - val_loss: 0.0058\n","Epoch 7/13\n","21586/21586 [==============================] - 54s 2ms/step - loss: 0.0080 - val_loss: 0.0059\n","Epoch 8/13\n","21568/21586 [============================>.] - ETA: 0s - loss: 0.0079\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","21586/21586 [==============================] - 56s 3ms/step - loss: 0.0079 - val_loss: 0.0056\n","Epoch 9/13\n","21586/21586 [==============================] - 56s 3ms/step - loss: 0.0074 - val_loss: 0.0055\n","Epoch 10/13\n","21568/21586 [============================>.] - ETA: 0s - loss: 0.0074\n","Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","21586/21586 [==============================] - 55s 3ms/step - loss: 0.0074 - val_loss: 0.0055\n","Epoch 11/13\n","21586/21586 [==============================] - 54s 3ms/step - loss: 0.0073 - val_loss: 0.0055\n","Epoch 12/13\n","21568/21586 [============================>.] - ETA: 0s - loss: 0.0073\n","Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","21586/21586 [==============================] - 50s 2ms/step - loss: 0.0073 - val_loss: 0.0055\n","Epoch 13/13\n","21586/21586 [==============================] - 53s 2ms/step - loss: 0.0073 - val_loss: 0.0055\n"],"name":"stdout"}]},{"metadata":{"collapsed":true,"id":"1W7MuIs1cxnk","colab_type":"code","colab":{}},"cell_type":"code","source":["save_obj(history.history,'./models_weights/_CNN_new6_76_new/history')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"htqL4Taxcxnm","colab_type":"code","outputId":"1e10c24f-f753-4258-d6c3-a524990540c4","colab":{}},"cell_type":"code","source":["plot_loss(history.history)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZIAAAEXCAYAAACH/8KRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHxd1L/zsVgwCAAAAAElFTkSuQmCC\n","text/plain":["<matplotlib.figure.Figure at 0x1fed0b17898>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"PQ2C7GsJcxno","colab_type":"text"},"cell_type":"markdown","source":["as we can see in this plot until epoch 8 as long as the train loss decrease the validation loss decrease as well, from this epoch the loss is constant and the model doesn't success to fit the train better."]},{"metadata":{"id":"r3Rg4Vkhcxno","colab_type":"text"},"cell_type":"markdown","source":["let's look at the rmse on the validation set:"]},{"metadata":{"id":"5WzSX62Rcxnp","colab_type":"code","outputId":"34b536f0-99a5-4f8b-c69e-6d7a1c8f09cb","colab":{}},"cell_type":"code","source":["model.load_weights('./models_weights/_CNN_new16_76_new/weights.11-0.01__CNN_new16_76_new.hdf5')\n","n_in=76\n","validation_X,validation_Y = to_supervised(validation_normalized,n_in)\n","nn_pred = model.predict(np.asarray(validation_X))\n","rmse(validation_Y,nn_pred)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.07419044963802596"]},"metadata":{"tags":[]},"execution_count":246}]},{"metadata":{"id":"j_X9L8W2cxnq","colab_type":"text"},"cell_type":"markdown","source":["this result looks great, but it depend on the loss of the validation set, we chose the model with the lowest validation lose. let's see if the situation with the rmse of the test set is similar:"]},{"metadata":{"id":"I_PH_va9cxnr","colab_type":"code","outputId":"42f906b5-4f73-4773-e928-3657d6b7f39e","colab":{}},"cell_type":"code","source":["X,Y = to_supervised(test_data_normalized,n_in)\n","rmse(Y,model.predict(np.asarray(X)))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.07668591072075313"]},"metadata":{"tags":[]},"execution_count":251}]},{"metadata":{"id":"3RBxAlW0cxns","colab_type":"text"},"cell_type":"markdown","source":["the rmse of the test set is not improve our last benchmark. this can be because of we chose the weights of the lowest loss, so the model we get is depend on the loss rate and has really nice loss and rmse result, but when we calculate rmse on test set that doesnt depend on any variable we see that this model fitted better the validation set than other models but it is not generalize emough because the rmse of the test set is higher than the previous models."]},{"metadata":{"id":"uWwBQ7CAcxns","colab_type":"code","outputId":"e855c677-3196-4574-f385-b52fb7ebd590","colab":{}},"cell_type":"code","source":["n_in=76; path = './models_weights/_CNN_new16_76_new/weights.11-0.01__CNN_new16_76_new.hdf5'\n","analyze_result(model,path,n_in)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hour</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>19</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>12</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>18</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>20</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>14</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>11</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>21</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>17</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>15</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>13</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>8</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>6</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>23</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>22</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>7</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    hour  count\n","0     19      8\n","1     12      7\n","2     18      6\n","3     20      4\n","4     14      4\n","5     11      4\n","6     21      3\n","7     17      2\n","8     15      2\n","9     13      2\n","10    10      2\n","11     8      2\n","12     6      2\n","13    23      1\n","14    22      1\n","15     7      1"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>pm</th>\n","      <td>33</td>\n","    </tr>\n","    <tr>\n","      <th>am</th>\n","      <td>18</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     0\n","pm  33\n","am  18"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>day</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Saturday</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sunday</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Wednesday</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Monday</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Thursday</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Tuesday</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Friday</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         day  count\n","0   Saturday     15\n","1     Sunday     10\n","2  Wednesday      9\n","3     Monday      8\n","4   Thursday      5\n","5    Tuesday      3\n","6     Friday      1"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"7LaHM7Jtcxnv","colab_type":"text"},"cell_type":"markdown","source":["total number of errors: 51  \n","at pm 64%  \n","at weekend 49%  \n","  \n","this model is not better than the previous models in this aspects and the using of cnn with lstm didn't help us in this case."]},{"metadata":{"id":"sLF9t6_ccxnx","colab_type":"text"},"cell_type":"markdown","source":["*** what can we do more?  ***  \n","the result we have got is pretty good, a good way to improve it can be by recognize patterns by training Conv1D and use the result as features for classic model. (we didn't try this because we did something similar in the last assignment and we wanted to try to get experience with different directions, and because in the last model it looks like conv1D doesnt help us.) or maybe try to get deeper and know better the data, in order to make feature engineering that can lead help the model to learn the data better."]},{"metadata":{"id":"VwiYK1TYcxny","colab_type":"text"},"cell_type":"markdown","source":["# SUMMARY  \n","we trained 6 different models, here we will write briefly the results on the test set:  "]},{"metadata":{"id":"bszk2Glqcxny","colab_type":"code","outputId":"abf94a7c-9b44-4d1f-c0eb-dc7d359bbbcd","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt; plt.rcdefaults()\n","import numpy as np\n","import matplotlib.pyplot as plt\n"," \n","objects = ('naive \\n baseline', 'linear \\n reression', 'lasso', 'simple \\n nn', 'feature \\n engineering nn', 'best n_in \\n nn', 'deep nn \\n with conv')\n","y_pos = np.arange(len(objects))\n","performance = [0.200,0.1197,0.122,0.0797,0.0763,0.0751,0.0766]\n","plt.figure(figsize=(10,5))\n"," \n","plt.bar(y_pos, performance, align='center', alpha=0.5)\n","plt.xticks(y_pos, objects)\n","plt.ylabel('RMSE')\n","plt.title('Model performance on test set')\n","\n","plt.show()\n"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA2MAAAHRCAYAAADnrGFeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njDHGmBL8P/AP3iiFPFiDAAAAAElFTkSuQmCC\n","text/plain":["<matplotlib.figure.Figure at 0x2625eb14940>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"AIiMxwQ6cxn0","colab_type":"code","outputId":"6a7c0393-8837-4109-8900-b7780ac6cf8f","colab":{}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n"," \n","# data to plot\n","n_groups = 4\n","pm = (0.68, 0.63, 0.7, 0.64)\n","weekend = (0.38, 0.4, 0.26, 0.49)\n","\n","# create plot\n","fig, ax = plt.subplots(figsize=(10,5))\n","index = np.arange(n_groups)\n","bar_width = 0.35\n","opacity = 0.8\n"," \n","rects1 = plt.bar(index, weekend, bar_width,\n","                 alpha=opacity,\n","                 color='b',\n","                 label='weekend')\n"," \n","rects2 = plt.bar(index + bar_width, pm, bar_width,\n","                 alpha=opacity,\n","                 color='g',\n","                 label='pm')\n","\n","plt.ylabel('% (precent)')\n","plt.title('precent of high rmse errors (0.2 above the model rmse) on the test set in weekend and pm')\n","plt.xticks(index + bar_width, ('simple nn', 'feature  \\n engineering nn', 'best n_in', 'deeper nn \\n with Conv1D'))\n","plt.legend() \n","plt.tight_layout()\n","\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA90AAAHqCAYAAAAZLi26AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAsQugGAAAAAMAihG4AAAAAACzy/wFqf6zle7xc3wAAAABJRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x2625cee5390>"]},"metadata":{"tags":[]}}]}]}